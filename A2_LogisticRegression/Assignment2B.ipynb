{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ibEsfuCyvJE"
   },
   "source": [
    "# Assignment 2B\n",
    "## Logistic regression,  interactions and generalized linear mixed models (GLMM)\n",
    "\n",
    "**In this second part of the assignment, we will see how to study binary decisions with multiple factors:**\n",
    "- using **logistic regression** (one form of Generalized Linear Model) to describe how decisions depend on a set of experimental factors (and their interactions)\n",
    "- using **generalized linear mixed models** (GLMM) to capture the heterogeneity of behavior across the population "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za5yFvk_yvJJ"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Preparing for this assignment:</b> \n",
    "\n",
    "The following resources will help you to get ready to complete this assignment. \n",
    "<ul>\n",
    "    <li>Watch <a href=\"https://www.youtube.com/watch?v=yIYKR4sgzI8\" target=\"_blank\">this video</a> to have a quick overview of linear and logistic regression, a form of generalized linear regression, and how they compare. You should learn: </li>\n",
    "    <ul>\n",
    "        <li>what type of data is appropriate for logistic regression modeling</li>\n",
    "        <li>how we can decide among candidate predictors using regression</li>\n",
    "        <li>regression fit methods: least-squares versus maximum likelihood</li>\n",
    "    </ul>\n",
    "    <li>Watch <a href=\"https://www.youtube.com/watch?v=vN5cNN2-HWE\" target=\"_blank\">this video</a> in order to understand the parameters of a logistic regression:</li>\n",
    "    <ul>\n",
    "        <li>concept of link function (<b>logit</b> for logistic regression)</li>\n",
    "        <li>how to interpret the intercept and slope parameters of predictors in a logistic regression</li>\n",
    "    </ul>\n",
    "    <li>Watch <a href=\"https://www.youtube.com/watch?v=HSHcIHMxhbE\" target=\"_blank\">this video</a> to see a step-by-step construction of a multiple linear model with interaction terms. Make sure you understand the logic of the interaction explained in the last half of the video. Take the exercise at the end of the video by stopping the video while you make your calculations.</li>\n",
    "    <li>Browse <a href=\"https://en.wikipedia.org/wiki/Interaction_(statistics)\" target=\"_blank\">this document</a> about interactions. It focuses a lot on ANOVAs, which we do not plan to address directly, so do not focus too much on that. I suggest reading the introduction, the <i>\"In regression\"</i> subsection and the <i>\"Examples\"</i> subsection. </li>\n",
    "    <li>OPTIONAL: If you were wondering why we skip ANOVAs and are happy just with linear models, ANOVA being such a popular statistical tool, <a href=\"https://eigenfoo.xyz/tests-as-linear/\" target=\"_blank\">here</a> is a more advanced Python document for you to make the point that linear models are a very general framework, which includes many of the classical statistical tests (also ANOVAs). We can run all our tests with linear models!!\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Af4ccUrk_2l"
   },
   "source": [
    "We will keep working on the same example as in part A of the assignment (psychometric curves).\n",
    "\n",
    "\n",
    "We start by importing the typical libraries, importing the data as a dataframe and adding the relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:37.824993Z",
     "start_time": "2022-02-09T22:27:36.401990Z"
    },
    "id": "2aF-qjnYk_2s"
   },
   "outputs": [],
   "source": [
    "#1. Load the packages that we will need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 2. load the data\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/wimmerlab/MBC-DataAnalysis/main/A2_LogisticRegression/Experiment1_all_subjects.csv\")\n",
    "\n",
    "# 3. Define a function that takes two angles as input and outputs the angular distance between the two\n",
    "def circdist_deg(angles1,angles2): #define the name and set the arguments between parentheses\n",
    "    angle_diff = angles2 - angles1 # simple difference\n",
    "    angle_diff_rad = angle_diff*np.pi/180 # convert to radians\n",
    "    angular_dist_rad = np.angle( np.exp(1j*(angle_diff_rad))) ## mathematical operation to get the circular distance (in radians)\n",
    "    angular_dist = 180/np.pi*angular_dist_rad ## convert back to degrees\n",
    "    output = np.round(angular_dist) # round value to eliminate numerical imprecisions (all values are integer)\n",
    "    return output #return the circular distance in degrees \n",
    "\n",
    "# 4. Compute the displacement of the probe from the target and add it as a new column to the dataframe: \n",
    "df['displacement'] = circdist_deg( df['target'], df['probe'] );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3WHlkjJk_3C"
   },
   "source": [
    "## 1. Simple logistic regression\n",
    "Logistic regression is one type of a larger family of statistical models called Generalized Linear Models (or GLMs) for binary data. Generalized Linear Models are a generalization of linear models (duh!) when the dependent variable is not a continuous variable (remember in Assignment 1 the dependent variable was an angle), for example: binary data, categorical data, count data, etc. In these cases, the assumption that observations correspond to the model plus some gaussian noise does not work. We will get back to GLMs when running regressions of spiking activity (Assignment 4).\n",
    "\n",
    "We thus model the data using function *glm* from the *statsmodel* package, with a binomial distribution (check the Python help). The binomial distribution simply means that we are observing binary data. \"GLM with binomial distribution\" is almost a synonym for \"logistic regression\" (side note: \"probit regression\" is another type of \"GLM with binomial distribution\" which is in practice is almost equivalent to logistic regression, as the only thing that changes is the exact shape of the sigmoidal function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-U0EvQaVyvJV"
   },
   "source": [
    "### 1.1 Fitting the simple logistic regression\n",
    "\n",
    "To apply logistic regression (or really any statistical model of binary data) we need the dependent variable response to be a binary variable, i.e. a list of 1's and 0's, not a list of 1's and -1's. We want our model to express the probability for a CCW response $p(CCW)$ so we need to change the convention in the field 'response' of the dataframe so that a CCW response is coded with a 1 (not a -1) and CW response as a 0 (not 1). <br>\n",
    "**Write the code to change the convention of the *response* field, so that CCWs are coded as 1s, and CWs as 0s** (check out the function loc in Pandas [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html), in the section *Setting values*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:37.881420Z",
     "start_time": "2022-02-09T22:27:37.862606Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "F4I0cWgwyvJX",
    "outputId": "e6fbd7c1-dfe3-43ed-b30c-574b605cbd65"
   },
   "outputs": [],
   "source": [
    "# change the column 'response': all -1's become 0's\n",
    "\n",
    "# check we did good\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:37.897912Z",
     "start_time": "2022-02-09T22:27:37.883783Z"
    },
    "id": "2ap_jA-WyvJZ"
   },
   "outputs": [],
   "source": [
    "# this should not output anything...\n",
    "assert df.response.sum()==1821\n",
    "assert np.round(np.sum(np.abs(df.displacement)))==21250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jm1FXeU1yvJb"
   },
   "source": [
    "We are now ready to run a simple logistic regression, where we consider the influence onto our binary variable (choices) of a single variable: the displacement of the probe w.r.t the target stimulus (the parameter that controls the difficulty of the trial). This is actually the exact same thing as fitting the psychometric curve! (see part A)\n",
    "\n",
    "First look up the function *glm* in the module *statsmodels* in Google to see how you can use it. <br>\n",
    "Then **fill the code to estimated this simple generalized linear model of choices with the displacement as predictor and the binomial family**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:38.970245Z",
     "start_time": "2022-02-09T22:27:37.900143Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjkbo6dTk_3F",
    "outputId": "cfd0e3bb-7d82-46d1-b0b6-8ad09244e602"
   },
   "outputs": [],
   "source": [
    "# import relevant functions from statsmodel package\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# declare the logistic regression model\n",
    "mod = smf.glm(formula= '??' , data= ?? , family= sm.families.Binomial() )\n",
    "\n",
    "# fit the model\n",
    "res = mod.fit()\n",
    "\n",
    "# print results as a summary table\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj7o1xyxyvJg"
   },
   "source": [
    "Interpret the table above. **What do we conclude about the intercept and the weight of the displacement?** <br>\n",
    "Compare the value of the weights and the Log-Likelihood with the maximum likelihood estimation that we performed in section 3 of Assignment 2A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pO2slM-yvJh"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:38.976339Z",
     "start_time": "2022-02-09T22:27:38.971803Z"
    },
    "id": "Tf59eYi_yvJi"
   },
   "outputs": [],
   "source": [
    "# this should not output anything...\n",
    "assert np.abs(np.round(1000*res.params[0]))==76\n",
    "assert np.abs(np.round(1000*res.params[1]))==138"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5j17XumIk_3R"
   },
   "source": [
    "## 1.2 Comparing model to data\n",
    "One important step whenever you model your data is to validate the model by assessing whether model captures the important features of your experimental data. Does the model provide a good fit? \n",
    "**Let us first look at the predicted values**, which can be obtained from the results of the model fit *res*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:39.369876Z",
     "start_time": "2022-02-09T22:27:38.979160Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "rE-KyAcfk_3T",
    "outputId": "26e62810-6226-4210-c7c3-65cc4335e8d8"
   },
   "outputs": [],
   "source": [
    "# predicted value for the model for each trial\n",
    "yfit = res.fittedvalues\n",
    "\n",
    "# plot it as displacement on the X axis, and predicted value (i.e. p(CCW)) on the y-axis\n",
    "??\n",
    "\n",
    "# don't forget axes labels!\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPF8lOh4k_3b"
   },
   "source": [
    "However, we would like to recover the full psychometric curve. That is, as in part A of the assignment, our statistical model can also predict the responses for many other displacements beyond the values that were tested. This is the predictive power of the model, what is exploited in machine learning. <br>\n",
    "**Let's plot the fitted psychometric curve**, i.e. the prediction curve for a linearly spaced list of x's (probe-target distances) in the range between -40 and 40 degrees. The fitted model can give you the predicted y's for those x's with the function `res.predict` in the results `res` of our model. Notice that because we defined our model with a formula, `predict` needs to get as argument also a Pandas DataFrame with a column *'displacement'* that contains the new *x* values on which we want to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:39.572084Z",
     "start_time": "2022-02-09T22:27:39.372597Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "QrOfY9cNk_3j",
    "outputId": "a119093b-ba18-40b5-c963-9a2bb6c64c51"
   },
   "outputs": [],
   "source": [
    "# array of linearly space values between -40 and 40 degrees\n",
    "myx = np.linspace(???)\n",
    "\n",
    "# create a new dataframe of synthetic data with these displacements\n",
    "df_synthetic = pd.DataFrame({??})\n",
    "\n",
    "# use the fitted statistical model to compute p(CCW) for each value in this dataframe\n",
    "yfit=res.predict(df_synthetic)\n",
    "\n",
    "# plot the psychometric curve\n",
    "??\n",
    "\n",
    "# don't forget axes labels!\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dMDcra0k_3w"
   },
   "source": [
    "To check how well the model fits the data, we can now plot on top of this prediction the percent of experimental responses for each possible distance between probe and target. \n",
    "In the previous assignment, we saw how we could plot the psychometric curve for experimental data by looking at the proportion of CCW responses separately for each level of displacement. Now we're going to exploit the full potental of dataframes by computing and plotting this psychometric curve in a single line (see below)! All the operations are computed sequentially in this line: grouping by level of displacement, selecting the response, average (and computing standard error of the mean), plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:39.784160Z",
     "start_time": "2022-02-09T22:27:39.574718Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "Y88u23srk_3y",
    "outputId": "a410ea27-1350-4456-8ed7-0774571475d0"
   },
   "outputs": [],
   "source": [
    "# COPY CODE HERE\n",
    "\n",
    "???????????\n",
    "\n",
    "# now adding psychometric curve from participants (in one line!!!)\n",
    "df.groupby('displacement').response.agg(('mean','sem')).plot(yerr='sem', fmt = 'ro', ax=plt.gca());\n",
    "\n",
    "# add legend\n",
    "plt.legend(('model','data'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IKGPXMjyvJl"
   },
   "source": [
    "**Does the model provide a good fit?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7J76VR3moL5"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1sTzPqpk_4D"
   },
   "source": [
    "## 2 Logistic regression with multiple factors\n",
    "### 2.1 With two factors\n",
    "Let us now include one further predictor, the distance between the target and the nearest non-target stimulus. This now goes in the direction of testing our hypothesis that non-target items kept in memory attract the memory of the target (as we tested in assignment 1).\n",
    "\n",
    "**Run again the logistic regression model adding the distance of the nearest distractor as a regressor.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:39.828914Z",
     "start_time": "2022-02-09T22:27:39.792156Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mdq3DIak_4F",
    "outputId": "bef95d12-eea5-4a82-aca0-a0bb96fa591e"
   },
   "outputs": [],
   "source": [
    "#Compute the distance between the distractor and the target and add as a new column to the dataframe:\n",
    "df['dist_distractor'] = circdist_deg( df['target'] , df['near'] )\n",
    "\n",
    "# we now run the GLM with two predictors, displacement and dist_distractor\n",
    "mod = smf.glm(????)\n",
    "res = mod.fit()\n",
    "\n",
    "print(res.summary())\n",
    "\n",
    "# print p-values for each predictor (check res.pvalues)\n",
    "print(\"p-value for the displacement predictor: p=\" + str(??)\n",
    "print(\"p-value for the dist_distractor predictor: p=\" + str(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:39.838188Z",
     "start_time": "2022-02-09T22:27:39.830686Z"
    },
    "id": "478ZGIm6yvJn"
   },
   "outputs": [],
   "source": [
    "assert np.round(np.abs(10000*res.params[2]))==3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does this additional predictor contributes significantly to the model description? How do you interpret this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyDmsGCvm2NR"
   },
   "source": [
    "### 2.2 With three factors\n",
    "Including the factor of interest \"distance from nearest non-target to target stimulus\" did not improve our model much, and the p-value for the coefficient of this factor is >>0.05, indicating that we cannot reject the null-hypothesis that this coefficient is zero. This is disappointing because the point of our experiment was to identify a biasing effect of simultaneous memories in reports in spatial working memory, but this model does not include all the detail about our experiment so we should go ahead and add all the other factors that intervened in our design and might interact with the putative biasing effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQIMUTFwk_4O"
   },
   "source": [
    "**We will now include one more predictor, the delay duration of each individual trial.  Run again the generalized linear model with this new regressor.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:39.896158Z",
     "start_time": "2022-02-09T22:27:39.840188Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgwwosnDk_4Q",
    "outputId": "cab4c9a3-ac02-4157-9c87-ef1bf3da7605"
   },
   "outputs": [],
   "source": [
    "# we include the predictor *delay* in the GLM\n",
    "mod = ?????\n",
    "res = ??\n",
    "\n",
    "print(res.summary())\n",
    "\n",
    "# print the p-values\n",
    "print(\"p-value for the displacement predictor: p=\" + str(??))\n",
    "print(\"p-value for the dist_distractor predictor: p=\" + str(??))\n",
    "print(\"p-value for the delay predictor: p=\" + str(??))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:39.902264Z",
     "start_time": "2022-02-09T22:27:39.898022Z"
    },
    "id": "yZ0IbCwFyvJq"
   },
   "outputs": [],
   "source": [
    "assert np.round(np.abs(10000*res.params[3]))==4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Does this last factor contributes significantly to the model description? What is your interpretation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZkfZP8YF4uF"
   },
   "source": [
    "Adding also the factor \"delay\" again only increased marginally the likelihood of our model and did not do this with a coefficient for *'delay'* significantly different from 0. However, if we think carefully about our intentions with this experiment, the crucial point is an interaction of interest that we have not yet included in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZqoE-M7k_4Y"
   },
   "source": [
    "### 2.3 Logistic regression including interaction of variables!\n",
    "\n",
    "Now notice that the hypothesis of our experiment was that there would be an attraction of behavioral responses toward near non-target memories, but specifically when there was a delay between cue and probe (i.e. this was a memory effect, as opposed to a perceptual effect). None of our analyses so far addressed this “specifically”, “only”, “here but not there” question. We need an interaction! Which one? <br>\n",
    "**Run the generalized linear model again adding the appropriate interaction terms and check if this interaction factor contributes significantly to the model description. What is your interpretation now?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:39.954296Z",
     "start_time": "2022-02-09T22:27:39.903947Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEC0Jt25k_4b",
    "outputId": "49b1298b-1746-487d-b7ef-79c48abd93fb"
   },
   "outputs": [],
   "source": [
    "#include the interaction of interest\n",
    "?????????????\n",
    "res=??\n",
    "\n",
    "print(res.summary())\n",
    "print(\"p-value for the probe_target factor: p=\" + str(??))\n",
    "print(\"p-value for the dist_NT factor: p=\" + str(??))\n",
    "print(\"p-value for the delay factor: p=\" + str(??))\n",
    "print(\"p-value for the interaction dist_NT*delay: p=\" + str(??))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:39.961324Z",
     "start_time": "2022-02-09T22:27:39.956133Z"
    },
    "id": "jDxg85o7yvJr"
   },
   "outputs": [],
   "source": [
    "assert np.round(np.abs(10000*res.params[4]))==7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPSFJdnSGZn9"
   },
   "source": [
    "Now we obtain a marginally significant interaction! When we have interactions, main effects are not easily interpretable since we can have effects for one predictor that go in different directions when considering different levels of another predictor, and this could yield vanishing main effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XobqWTQ3k_4o"
   },
   "source": [
    "### 2.4 Understanding significant interactions\n",
    "Significant interactions can be analyzed by separating by one of the implicated predictors. <br>\n",
    "**Run the model without the interaction factor and without the delay factor (i.e. the model of 2.1) separately on trials with delay 0.1 second (*perceptual trials*), and on trials with delays 1 and 3 seconds (*memory trials*)**. What do you see? \n",
    "\n",
    "Hint: Take special attention not only to the significance of the regressors, but also to the value of the fitted beta parameter (in particular, the sign). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:40.033128Z",
     "start_time": "2022-02-09T22:27:39.962921Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G2AgDAdnk_4q",
    "outputId": "67c70740-b8de-4769-add5-c2439a255f77"
   },
   "outputs": [],
   "source": [
    "print('LOGISTIC REGRESSION IN PERCEPTUAL TRIALS')\n",
    "\n",
    "#we build a boolean to select trials with delay 0.1 \n",
    "bool_percept = ?? \n",
    "\n",
    "# we run now the two-regressor model one this subset of data, including only perceptual trials\n",
    "??????\n",
    "res =?? # fit the model\n",
    "\n",
    "# print summary\n",
    "print(res.summary())\n",
    "print(\"parameter for the dist_distractor predictor: beta = \" + str(res.params[??]))\n",
    "print(\"p-value for the dist_distractor predictor: p=\" + str(res.pvalues[??]))\n",
    "\n",
    "########\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('LOGISTIC REGRESSION IN MEMORY TRIALS')\n",
    "\n",
    "# copy and adapt the code above\n",
    "??????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:40.038753Z",
     "start_time": "2022-02-09T22:27:40.035110Z"
    },
    "id": "Mv4-BtR8yvJs"
   },
   "outputs": [],
   "source": [
    "assert np.round(np.abs(10000*res.params[2]))==12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6W-yjevG57x"
   },
   "source": [
    "Now that we split the data between perceptual trials and memory trials (i.e. short and long delays), we obtain that the two-regressor model now shows a significant main effect for the *dist_distractor* factor, in memory trials only. This was previously non-significant in the full model. If we look closely, we will see that the value of the coefficient for *dist_distractor* is now positive for short delays (0.0013)  and negative for long delays (-0.0012). This is indicating that the bias induced by non-targets may be different for each of these 2 conditions: repulsive for short delays (or non-significant), and attractive for long delays. This is the interaction that we found significant and this would explain that when we run the full model the main factor *dist_distractor* has a vanishing coefficient because attractions and repulsions form the different delay conditions cancel one another.\n",
    "\n",
    "We validate this intuition by plotting the data graphically in a way that will reveal our interpretation. This step is crucial to consolidate our interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:40.615753Z",
     "start_time": "2022-02-09T22:27:40.040452Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "D4tvEM55yvJt",
    "outputId": "f5a8fa2b-e85d-4996-8d33-56e59fbbf732"
   },
   "outputs": [],
   "source": [
    "# add boolean vector that is true for memory trials only (i.e. delay = 1 or 3 seconds)\n",
    "df['memory'] = ??  #this dichotomizes the delay: 0=perceptual trial, 1=memory trial\n",
    "\n",
    "# create a subplot to the left\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "# 1. FIRST FIT AND PLOT FOR PERPCETUAL TRIALS WITH CCW DISTRACTORS\n",
    "\n",
    "# boolean that is true for short delay trials with CCW distractors\n",
    "bool_sel = ??\n",
    "\n",
    "# fit single logistic regression (section 1) on these trials only\n",
    "????\n",
    "\n",
    "# define 100 angle points linearly spaced between -20 and 20 degrees\n",
    "myx = ??\n",
    "\n",
    "# compute predicted value for each of these values\n",
    "yfit=??\n",
    "\n",
    "# plot fitted psychometric functions (in blue)\n",
    "plt.plot(????, label=\"CCW\")\n",
    "\n",
    "#now this adds the psychometric curve for experimental data (in blue)\n",
    "df[bool_sel].groupby('displacement').response.agg(('mean','sem')).plot(yerr='sem', fmt = 'bo', ax=plt.gca(), legend=False);\n",
    "\n",
    "# NOW REPEAT THE SAME FOR SHORT DELAYS TRIALS WITH CW DISTRACTOR\n",
    "\n",
    "?????\n",
    "\n",
    "###################### ADD NEW SUBPLOT\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "# REPEAT THE SAME FOR MEMORY TRIALS (FIRST MEMORY TRIALS WITH CCW DISTRACTOR, THEN MEMORY TRIALS WITH CW DISTRACTOR)\n",
    "??????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SghVm7sUyvJt"
   },
   "source": [
    "#### OPTIONAL: \n",
    "(*In the cell above we repeated a lot of code. We could improve our Notebook by defining a function that plots one curve and data points, given the model and the original data, and then call this function 4 times as you select the various subplots. Go ahead and try this, if you like*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:40.629922Z",
     "start_time": "2022-02-09T22:27:40.618634Z"
    },
    "id": "Wd_5p4GFyvJu"
   },
   "outputs": [],
   "source": [
    "# Function that takes two arguments (a dataframe and a color, and plot the fitting psychometric curve with experimental datapoints on top)\n",
    "def plotcurve(df, color):\n",
    "    mod = smf.glm(formula='response ~ displacement', data=df, family=sm.families.Binomial())\n",
    "    res = mod.fit()\n",
    "    \n",
    "    myx = np.linspace(-20,20,100)\n",
    "    yfit=res.predict(pd.DataFrame({'displacement': myx})) #yfit = res.predict(exog={'probe_target':myx})\n",
    "    plt.plot(myx,yfit,'-', color=color)\n",
    "    \n",
    "    df.groupby('displacement').response.agg(('mean','sem')).plot(yerr='sem', color=color, fmt = 'o', ax=plt.gca());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:41.191357Z",
     "start_time": "2022-02-09T22:27:40.632363Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "bh3Ta-_KyvJv",
    "outputId": "29cb3af2-1f6f-462b-d439-15c3a44425e6"
   },
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "\n",
    "# we fit and plot first short delays for CCW trials\n",
    "bool_sel = ???\n",
    "plotcurve(df[bool_sel],'b')\n",
    "\n",
    "bool_sel = ???\n",
    "plotcurve(df[bool_sel],'r')\n",
    "\n",
    "plt.legend(('CCW distractor','CW distractor'))\n",
    "plt.title('short delays')\n",
    "plt.ylabel('p(CCW)')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "# we fit and plot long delays for CCW trials (SAME AS ABOVE BUT FOR MEM, CCW TRIALS)\n",
    "bool_sel = ??\n",
    "plotcurve(df[bool_sel],'b')\n",
    "bool_sel = ??\n",
    "plotcurve(df[bool_sel],'r')\n",
    "\n",
    "plt.legend(('CCW distractor','CW distractor'))\n",
    "plt.title('long delays');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_km4lAzk_49"
   },
   "source": [
    "## 2. Population analysis\n",
    "### 2.1 Adding participant as categorical variable\n",
    "\n",
    "In assignment 1 (linear regression), we ran a regression analysis on the data collapsing together data from all subjects and we then went on to run a regression analysis incorporating subjects as factors. We now want to incorporate the subject factor into our analysis, but notice that now this factor is categorical, not parametric as all other factors previously. In regression models, categorical predictors need special treatment: we use *C(factor)* to specify that *factor* is categorical. \n",
    "\n",
    "Run the **logistic regression model of 2.4 adding subject as an extra regressor**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:41.273808Z",
     "start_time": "2022-02-09T22:27:41.193347Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JmiErUzPyvJw",
    "outputId": "712c86ee-1e13-4e6d-a8e7-f94cafad4a02"
   },
   "outputs": [],
   "source": [
    "# define model adding one intercept per subject\n",
    "????????\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:41.283630Z",
     "start_time": "2022-02-09T22:27:41.279636Z"
    },
    "id": "ZIJlOMbeyvJw"
   },
   "outputs": [],
   "source": [
    "assert np.round(np.abs(1000*res.params[5]))==327"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2kJCPF5yvJx"
   },
   "source": [
    "**How do you interpret the weights for each subject?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F7VVWeiyvJx"
   },
   "source": [
    "Each weight corresponds to a shift in the proportion of CCW responses specific to that subject, i.e. it represents the bias towards CCW responses for each subject. Note that there is no weight for subject 1, which is the reference subject. We always need a reference value for categorical regressors (to avoid colinearity). The bias of subject 1 is given by the intercept. So the bias for other subjects is actually the intercept PLUS their subject-specific weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4U_a6AvmyvJy"
   },
   "source": [
    "**Does the fit improve?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOqI0w9AyvJy"
   },
   "source": [
    "The log-likelihood of the model increased from -2152 to -2147 so there's a slight improvement in the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L56vYqOZyvJy"
   },
   "source": [
    "### 3.2 Generalized linear mixed model (GLMM)\n",
    "Here, we treated participants as fixed effects of our design, and this is good to make inferences about our sample population. However, to make more general inferences on the larger population that our sample represents, we should treat the subject factor as a random effect. This would then correspond to a Generalized Linear Mixed Model (GLMM). The terms *mixed* reflects the fact that the model includes both *fixed effects* (effects that are stable in the population, generally the ones we're interested in) and *random effects* (effects that vary between individuals). GLMMs are advanced regression methods implemented in the *lmer* function from the *pymer4* package. However there are many more packages for running these models in R, and here we'll see how to run R from Python. You can learn about this on your own, it is not part of the assignment, but you can play below to try to run such a model on this data (this is what was done in the orginal article by Almeida et al 2015)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NpVrauTCyvJz",
    "outputId": "7d60fa6d-078c-4bca-e0a4-fcca8e3456b6"
   },
   "outputs": [],
   "source": [
    "# WARNING: THIS TAKES ABOUT 3~5 minutes (and will output a lot of text)\n",
    "# Install R and Rpy2\n",
    "!apt-get install r-base\n",
    "#!pip install -q rpy2\n",
    "!pip install rpy2==3.5.1\n",
    "\n",
    "# Install LMER packages (for R)\n",
    "packnames = ('lme4', 'lmerTest', 'emmeans')\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "utils = importr(\"utils\")\n",
    "utils.chooseCRANmirror(ind=1)\n",
    "utils.install_packages(StrVector(packnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2k9H_2-KyvJ0"
   },
   "outputs": [],
   "source": [
    "# use R packages in python code. You have to have R installed, and the specific packages also need to be installed in R\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aiNsp1gJ5P9l"
   },
   "outputs": [],
   "source": [
    "# load packages in R (actually we probably don't use all of them)\n",
    "base     = importr('base')\n",
    "stats    = importr('stats')\n",
    "lme4     = importr('lme4')\n",
    "scales   = importr('scales')\n",
    "lmerTest = importr('lmerTest')\n",
    "\n",
    "# create R dataframe for mixed models from our Python dataframe\n",
    "with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "  ro.globalenv['rdat'] = ro.conversion.py2rpy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BEfdnFwQ72EY",
    "outputId": "533a59bb-749c-495b-f608-ef89ab78cde4"
   },
   "outputs": [],
   "source": [
    "# define and fit the model in R. The command in R is placed in ro.r function: r0.r('Rcommand')\n",
    "m0 = ro.r('glmm = lmer(response ~ displacement + dist_distractor*memory + (1|subject), data=rdat)')\n",
    "\n",
    "# print model summary\n",
    "print(lmerTest.summary_lmerModLmerTest(m0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eWk5ombyvJ1"
   },
   "source": [
    "This model only considered subject-to-subject variability coming from systematic CW/CCW biases in the responses (the term `(1|subj)` considers variability in the intercept, which determines CW/CCW biases in the responses of individual participants). However, this is maybe not the most intuitive way that we expect our participants to differ from one another. It makes maybe more sense to think that what will really be different from subject to subject is the sensibility in detecting small displacements of the probe. Some people will be very sensitive to small displacements, while others will need longer distances to perceive clearly the shift. In other words, some subjects will discriminate better than others. The sensibility is the maximal slope of the psychometric function, and it is directly related to the logistic regression parameter for the `displacement` variable. Here is how would you enter this subject-to-subject variability in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T22:27:54.433510Z",
     "start_time": "2022-02-09T22:27:50.395203Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpRXJ7hVyvJ2",
    "outputId": "93527dfb-add3-4de3-afd8-7aed5bb1cc9c",
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "m0 = ro.r('glmm = lmer(response ~ displacement + dist_distractor*memory + (1+displacement|subject), data=rdat)')\n",
    "print(lmerTest.summary_lmerModLmerTest(m0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IB29HyaKyvJ2"
   },
   "source": [
    "This is the proper way of running the analysis. The random factor considers variability between participants in mean responses and in the sensitivity to `probe_target`, and establishes the pairing between data collected for each participant. This analysis reveals that the `dist_NT:delay` interaction is significant, with `p=0.012`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vX1PqLOmyvJ3"
   },
   "source": [
    "An important element that has been added here as well is the dichotomization of the delays. Instead of asking effects to be 3 times stronger in 3s-delay trials compared to 1s-delay trials, we just focus on differences between delay trials (either 1s or 3s trials, both with score 1) and no-delay trials (0.1s delay, now scored 0). You can try and see that this also improves the significance of the interaction term in the previous analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-09T22:31:18.523Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B7FTnYP4yvJ3",
    "outputId": "5e72e45f-6f43-4fec-95e7-3f977e932688"
   },
   "outputs": [],
   "source": [
    "m0 = ro.r('glmm = lmer(response ~ displacement*dist_distractor*memory + (1+displacement|subject), data=rdat)')\n",
    "print(lmerTest.summary_lmerModLmerTest(m0))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "assignment5.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
