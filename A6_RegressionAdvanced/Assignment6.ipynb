{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pxFPNerzM0Bh"
   },
   "source": [
    "### Assignment 6: Advanced topics in regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this assignment, we will:**\n",
    "- study how the estimated weights of a regression model change when introducing a correlated regressor, and understand what that means for **introducing nuisance regressors**\n",
    "- study why a **regression model cannot include colinear regressors**\n",
    "- see how we **detect colinear or strongly correlated regressors** that could affect estimation of weights, using the **Variance Inflation Factor**\n",
    "- study how we can determine in advance if our sample size is adequate to find a statistical significant effect, using **simulation-based power analysis**\n",
    "- see how to take the heterogenity of behavior across participants into account, using **linear mixed models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Preparing for this assignment:</b> \n",
    "All concepts will be discussed in class, you can access the slides for linear mixed models <a href=\"https://github.com/wimmerlab/MBC-DataAnalysis/blob/21b0b5c843c036258884dfefd537b23b7a5bda34/A6_RegressionAdvanced/BAMB!%20Tutorial%209%20-%20Linear%20Mixed%20Models.pdf\">here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the numerical judgment dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first use the **same dataset as in Assignment 1** (Simple statistics): the orientation judgment dataset of (Talluri et al.,*Current Biology*). Reminder: the paradigm is presented in Figure 1 of the [article](https://www.cell.com/action/showPdf?pii=S0960-9822%2818%2930982-5). The structure of each trial is the following:\n",
    "- first, a stimulus (here a sequence of oriented gratings)\n",
    "- a discrimination task on this first stimulus (is the sequence overall tilted more clockwise or counter-clockwise?)\n",
    "- then a second stimulus (another sequence of oriented gratings)\n",
    "- finally, a numerical judgment task (report the mean over the *two* sequences).\n",
    "\n",
    "Load the data as a Pandas dataframe (as in Assignment 1). `x1` now represents the average orientation in the first sequence, and correspondingly for `x2`. In this dataset we will ignore the impact of the discrimination task on the final judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>xavg</th>\n",
       "      <th>estim</th>\n",
       "      <th>subj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10</td>\n",
       "      <td>0</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5.21220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-10</td>\n",
       "      <td>-10</td>\n",
       "      <td>-10</td>\n",
       "      <td>-8.21768</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>-17.93416</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-20</td>\n",
       "      <td>10</td>\n",
       "      <td>-5</td>\n",
       "      <td>-13.42634</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>-10</td>\n",
       "      <td>5</td>\n",
       "      <td>9.88556</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  xavg     estim  subj\n",
       "0 -10   0    -5  -5.21220     1\n",
       "1 -10 -10   -10  -8.21768     1\n",
       "2  20  10    15 -17.93416     1\n",
       "3 -20  10    -5 -13.42634     1\n",
       "4  20 -10     5   9.88556     1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import typical packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# load the data from github\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/wimmerlab/MBC-DataAnalysis/main/A1_Statistics/Rawdata_Perceptual_simple.csv\",sep=',')\n",
    "\n",
    "# uncomment if you prefer to load the data locally\n",
    "#df = pd.read_csv(\"../A1_Statistics\\Rawdata_Perceptual_simple.csv\",sep=',')\n",
    "\n",
    "#Â list of subjects\n",
    "subjects = np.unique(df.subj)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The impact of introducing correlated regressors\n",
    "We want to understand how regression weights for a certain regressor $x1$ changes when we include into the model another regressor $x2$ that correlates with $x1$.\n",
    "We will illustrate that where $x1$ and $x2$ correspond to the two stimulus sequences in each trial (and the dependent variable is the orientation estimated by the subject). We will compare the model with the second stimulus as unique regressor with the model where both stimuli are included.\n",
    "But first, **let's check that these two stimuli are indeed correlated, using Pearson's correlation.** We will use `pearsonr` from the `scipy` package (note: `numpy` has its own function for computing the correlation - `corrcoef` - but it does not provide p-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's correlation coefficient 0.19056305211467897\n",
      "Pearson's correlation p-value 5.600404569562593e-118\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# compute Pearson's correlation coeff and corresponding p-values\n",
    "rho, corr_p = pearsonr(??,??)\n",
    "\n",
    "print(\"Pearson's correlation coefficient\", ??)\n",
    "print(\"Pearson's correlation p-value\", ??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Are the first and second stimuli correlated? Why do you think that stimuli were generated that way?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The correlation is positive ($\\rho \\approx 0.19$) and highly significant. This is because both stimulus sequences are generated from the same underlying distribution, which is either clockwise or counter-clockwise.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's fit the linear regression model with both stimuli as regressors.** We will make the comparison with the data of a single subject, subject 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:46:53.929474Z",
     "start_time": "2022-02-04T22:46:53.866909Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dONKI852M0Ed",
    "outputId": "d9e824c3-8060-443d-f4ad-5295ab98e8d2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>estim</td>      <th>  R-squared:         </th> <td>   0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   58.28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 22 Feb 2024</td> <th>  Prob (F-statistic):</th> <td>1.05e-24</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:09:39</td>     <th>  Log-Likelihood:    </th> <td> -4150.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1034</td>      <th>  AIC:               </th> <td>   8307.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1031</td>      <th>  BIC:               </th> <td>   8322.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    3.7653</td> <td>    0.417</td> <td>    9.024</td> <td> 0.000</td> <td>    2.947</td> <td>    4.584</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>        <td>    0.3311</td> <td>    0.031</td> <td>   10.518</td> <td> 0.000</td> <td>    0.269</td> <td>    0.393</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>        <td>    0.0117</td> <td>    0.031</td> <td>    0.371</td> <td> 0.711</td> <td>   -0.050</td> <td>    0.073</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>241.912</td> <th>  Durbin-Watson:     </th> <td>   1.964</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>  66.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.368</td>  <th>  Prob(JB):          </th> <td>4.13e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.001</td>  <th>  Cond. No.          </th> <td>    14.7</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  estim   R-squared:                       0.102\n",
       "Model:                            OLS   Adj. R-squared:                  0.100\n",
       "Method:                 Least Squares   F-statistic:                     58.28\n",
       "Date:                Thu, 22 Feb 2024   Prob (F-statistic):           1.05e-24\n",
       "Time:                        12:09:39   Log-Likelihood:                -4150.5\n",
       "No. Observations:                1034   AIC:                             8307.\n",
       "Df Residuals:                    1031   BIC:                             8322.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      3.7653      0.417      9.024      0.000       2.947       4.584\n",
       "x1             0.3311      0.031     10.518      0.000       0.269       0.393\n",
       "x2             0.0117      0.031      0.371      0.711      -0.050       0.073\n",
       "==============================================================================\n",
       "Omnibus:                      241.912   Durbin-Watson:                   1.964\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               66.241\n",
       "Skew:                          -0.368   Prob(JB):                     4.13e-15\n",
       "Kurtosis:                       2.001   Cond. No.                         14.7\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import OLS from statsmodel package\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# select dataframe for subject 14\n",
    "df_singlesubj = ???\n",
    "\n",
    "# define model formula and database (point to a dataframe)\n",
    "mod = ???\n",
    "\n",
    "# fit the model\n",
    "res = mod.fit()\n",
    "\n",
    "# summary\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **fit the model with the second stimulus as unique regressor**, again for subject 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>estim</td>      <th>  R-squared:         </th> <td>   0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 22 Feb 2024</td> <th>  Prob (F-statistic):</th>  <td>0.0209</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:09:39</td>     <th>  Log-Likelihood:    </th> <td> -4203.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1034</td>      <th>  AIC:               </th> <td>   8410.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1032</td>      <th>  BIC:               </th> <td>   8420.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    3.7583</td> <td>    0.439</td> <td>    8.564</td> <td> 0.000</td> <td>    2.897</td> <td>    4.620</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>        <td>    0.0751</td> <td>    0.032</td> <td>    2.314</td> <td> 0.021</td> <td>    0.011</td> <td>    0.139</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>3035.567</td> <th>  Durbin-Watson:     </th> <td>   1.990</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 121.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>-0.520</td>  <th>  Prob(JB):          </th> <td>4.97e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 1.685</td>  <th>  Cond. No.          </th> <td>    13.5</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  estim   R-squared:                       0.005\n",
       "Model:                            OLS   Adj. R-squared:                  0.004\n",
       "Method:                 Least Squares   F-statistic:                     5.353\n",
       "Date:                Thu, 22 Feb 2024   Prob (F-statistic):             0.0209\n",
       "Time:                        12:09:39   Log-Likelihood:                -4203.2\n",
       "No. Observations:                1034   AIC:                             8410.\n",
       "Df Residuals:                    1032   BIC:                             8420.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      3.7583      0.439      8.564      0.000       2.897       4.620\n",
       "x2             0.0751      0.032      2.314      0.021       0.011       0.139\n",
       "==============================================================================\n",
       "Omnibus:                     3035.567   Durbin-Watson:                   1.990\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              121.133\n",
       "Skew:                          -0.520   Prob(JB):                     4.97e-27\n",
       "Kurtosis:                       1.685   Cond. No.                         13.5\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the estimated values and p-values of the second stimulus regressor between the two models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The second stimulus regressor is not significant in the full model (value =0.0117, p>0.7). It is much larger (value = 0.0751) and significant (p=0.02) in the single-regressor model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why did this happen?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The weight is inflated in the single-regressor model as it corresponds both to the direct impact of the second stimulus onto the participant estimate, and to the impact of the first stimulus which is not included in the model (and does correlate with the second). In the fuller model, the impact of the first stimulus is directly explained by the corresponding regressor, so the weight for the second stimulus corresponds to the direct impact only (which is what we want). This illustrates why **we always need to include variables that are correlated with our variable of interest into a regression model**, even when the impact of these variables are not of interest to us (\"nuisance regressor\"). This avoids spurious significant effects like the one we found here for subject 14 (significant p-value in the single-regressor model, yet the real impact of the second stimulus is not significant).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Colinear regressors\n",
    "We now turn to the problem of colinear regressors. A set of $n$ regressors is said to be colinear if there is a **linear combination of them that always gives 0**, for all trials (observations). For example if regressors $(x_1,x_2,x_3)$ (out of all regressor models) are such that $x_1 + 2x_2- 4x_3=0$. Basically it means that these **regressors encode redundant information**. This is a serious problem because there is an equivalence between different sets of weights (actually each set of weights is equivalent to an infinity of other sets), so we cannot provide unique estimates of the weights. The model is then said to be **unidentifiable**.\n",
    "This is illustrated below, where we want to see the impact of the average stimulus (`xavg`) separately for different bins of the value. Such binning is useful for example if we want to check that the impact of the stimulus is really linear, as implicitly assumed if we declare the model ` estim ~ xavg`.\n",
    "\n",
    "First let us bin the values of the average stimulus into 6 different bins, using the function `cut` (in pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>xavg</th>\n",
       "      <th>estim</th>\n",
       "      <th>subj</th>\n",
       "      <th>x_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10</td>\n",
       "      <td>0</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5.21220</td>\n",
       "      <td>1</td>\n",
       "      <td>(-6.667, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-10</td>\n",
       "      <td>-10</td>\n",
       "      <td>-10</td>\n",
       "      <td>-8.21768</td>\n",
       "      <td>1</td>\n",
       "      <td>(-13.333, -6.667]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>-17.93416</td>\n",
       "      <td>1</td>\n",
       "      <td>(13.333, 20.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-20</td>\n",
       "      <td>10</td>\n",
       "      <td>-5</td>\n",
       "      <td>-13.42634</td>\n",
       "      <td>1</td>\n",
       "      <td>(-6.667, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>-10</td>\n",
       "      <td>5</td>\n",
       "      <td>9.88556</td>\n",
       "      <td>1</td>\n",
       "      <td>(0.0, 6.667]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  xavg     estim  subj              x_bin\n",
       "0 -10   0    -5  -5.21220     1      (-6.667, 0.0]\n",
       "1 -10 -10   -10  -8.21768     1  (-13.333, -6.667]\n",
       "2  20  10    15 -17.93416     1     (13.333, 20.0]\n",
       "3 -20  10    -5 -13.42634     1      (-6.667, 0.0]\n",
       "4  20 -10     5   9.88556     1       (0.0, 6.667]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of bins\n",
    "nBin = 6\n",
    "\n",
    "# add a new variable with binned values (the bin boundaries are defined automatically)\n",
    "df['x_bin'] = pd.cut(df.xavg,nBin)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to use these values as regressors, let's **create a different binary variable corresponding to each of these bins** (so that for example `bin1`=1 if the value `xavg` falls in the first bin, 0 otherwise). What we are doing is called *one-hot encoding*: defining one binary variable for each value of the categorical variable `x_bin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>xavg</th>\n",
       "      <th>estim</th>\n",
       "      <th>subj</th>\n",
       "      <th>x_bin</th>\n",
       "      <th>bin1</th>\n",
       "      <th>bin2</th>\n",
       "      <th>bin3</th>\n",
       "      <th>bin4</th>\n",
       "      <th>bin5</th>\n",
       "      <th>bin6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10</td>\n",
       "      <td>0</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5.21220</td>\n",
       "      <td>1</td>\n",
       "      <td>(-6.667, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-10</td>\n",
       "      <td>-10</td>\n",
       "      <td>-10</td>\n",
       "      <td>-8.21768</td>\n",
       "      <td>1</td>\n",
       "      <td>(-13.333, -6.667]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>-17.93416</td>\n",
       "      <td>1</td>\n",
       "      <td>(13.333, 20.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-20</td>\n",
       "      <td>10</td>\n",
       "      <td>-5</td>\n",
       "      <td>-13.42634</td>\n",
       "      <td>1</td>\n",
       "      <td>(-6.667, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>-10</td>\n",
       "      <td>5</td>\n",
       "      <td>9.88556</td>\n",
       "      <td>1</td>\n",
       "      <td>(0.0, 6.667]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-20</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>-14.17070</td>\n",
       "      <td>1</td>\n",
       "      <td>(-13.333, -6.667]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-20</td>\n",
       "      <td>-20</td>\n",
       "      <td>-20</td>\n",
       "      <td>-8.83053</td>\n",
       "      <td>1</td>\n",
       "      <td>(-20.04, -13.333]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>15.12759</td>\n",
       "      <td>1</td>\n",
       "      <td>(0.0, 6.667]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-20</td>\n",
       "      <td>10</td>\n",
       "      <td>-5</td>\n",
       "      <td>-11.17976</td>\n",
       "      <td>1</td>\n",
       "      <td>(-6.667, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-10</td>\n",
       "      <td>-20</td>\n",
       "      <td>-15</td>\n",
       "      <td>-12.24974</td>\n",
       "      <td>1</td>\n",
       "      <td>(-20.04, -13.333]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  xavg     estim  subj              x_bin  bin1  bin2  bin3  bin4  \\\n",
       "0 -10   0    -5  -5.21220     1      (-6.667, 0.0]     0     0     1     0   \n",
       "1 -10 -10   -10  -8.21768     1  (-13.333, -6.667]     0     1     0     0   \n",
       "2  20  10    15 -17.93416     1     (13.333, 20.0]     0     0     0     0   \n",
       "3 -20  10    -5 -13.42634     1      (-6.667, 0.0]     0     0     1     0   \n",
       "4  20 -10     5   9.88556     1       (0.0, 6.667]     0     0     0     1   \n",
       "5 -20   0   -10 -14.17070     1  (-13.333, -6.667]     0     1     0     0   \n",
       "6 -20 -20   -20  -8.83053     1  (-20.04, -13.333]     1     0     0     0   \n",
       "7   0  10     5  15.12759     1       (0.0, 6.667]     0     0     0     1   \n",
       "8 -20  10    -5 -11.17976     1      (-6.667, 0.0]     0     0     1     0   \n",
       "9 -10 -20   -15 -12.24974     1  (-20.04, -13.333]     1     0     0     0   \n",
       "\n",
       "   bin5  bin6  \n",
       "0     0     0  \n",
       "1     0     0  \n",
       "2     0     1  \n",
       "3     0     0  \n",
       "4     0     0  \n",
       "5     0     0  \n",
       "6     0     0  \n",
       "7     0     0  \n",
       "8     0     0  \n",
       "9     0     0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels for all of the bins\n",
    "bin_label = ??\n",
    "\n",
    "# for each bin\n",
    "for i, bin in enumerate(bin_label):\n",
    "\n",
    "    # name of the new variable\n",
    "    lbl = \"bin\"+str(i+1)\n",
    "    \n",
    "    # define variable with boolean condition (and multiply by one to convert to 0/1)\n",
    "    df[lbl] = 1*(??)\n",
    "    \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a second to check that the variables have been correctly encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pretend to be naive and **fit a linear regression model with all of these binary variables as regressors** (using all subjects at once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>estim</td>      <th>  R-squared:         </th> <td>   0.033</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.033</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   98.69</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 22 Feb 2024</td> <th>  Prob (F-statistic):</th> <td>1.23e-102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:09:39</td>     <th>  Log-Likelihood:    </th> <td> -65698.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 14418</td>      <th>  AIC:               </th> <td>1.314e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 14412</td>      <th>  BIC:               </th> <td>1.315e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.9386</td> <td>    0.173</td> <td>    5.431</td> <td> 0.000</td> <td>    0.600</td> <td>    1.277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin1</th>      <td>   -6.8748</td> <td>    0.481</td> <td>  -14.281</td> <td> 0.000</td> <td>   -7.818</td> <td>   -5.931</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin2</th>      <td>   -4.0182</td> <td>    0.481</td> <td>   -8.351</td> <td> 0.000</td> <td>   -4.961</td> <td>   -3.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin3</th>      <td>   -1.4375</td> <td>    0.341</td> <td>   -4.213</td> <td> 0.000</td> <td>   -2.106</td> <td>   -0.769</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin4</th>      <td>    1.9072</td> <td>    0.426</td> <td>    4.476</td> <td> 0.000</td> <td>    1.072</td> <td>    2.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin5</th>      <td>    4.7267</td> <td>    0.481</td> <td>    9.823</td> <td> 0.000</td> <td>    3.784</td> <td>    5.670</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin6</th>      <td>    6.6353</td> <td>    0.481</td> <td>   13.783</td> <td> 0.000</td> <td>    5.692</td> <td>    7.579</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>5050.296</td> <th>  Durbin-Watson:     </th>  <td>   1.754</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>292783.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.878</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>25.006</td>  <th>  Cond. No.          </th>  <td>3.85e+15</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.17e-27. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  estim   R-squared:                       0.033\n",
       "Model:                            OLS   Adj. R-squared:                  0.033\n",
       "Method:                 Least Squares   F-statistic:                     98.69\n",
       "Date:                Thu, 22 Feb 2024   Prob (F-statistic):          1.23e-102\n",
       "Time:                        12:09:39   Log-Likelihood:                -65698.\n",
       "No. Observations:               14418   AIC:                         1.314e+05\n",
       "Df Residuals:                   14412   BIC:                         1.315e+05\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.9386      0.173      5.431      0.000       0.600       1.277\n",
       "bin1          -6.8748      0.481    -14.281      0.000      -7.818      -5.931\n",
       "bin2          -4.0182      0.481     -8.351      0.000      -4.961      -3.075\n",
       "bin3          -1.4375      0.341     -4.213      0.000      -2.106      -0.769\n",
       "bin4           1.9072      0.426      4.476      0.000       1.072       2.742\n",
       "bin5           4.7267      0.481      9.823      0.000       3.784       5.670\n",
       "bin6           6.6353      0.481     13.783      0.000       5.692       7.579\n",
       "==============================================================================\n",
       "Omnibus:                     5050.296   Durbin-Watson:                   1.754\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           292783.194\n",
       "Skew:                           0.878   Prob(JB):                         0.00\n",
       "Kurtosis:                      25.006   Cond. No.                     3.85e+15\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.17e-27. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model formula and database (point to a dataframe)\n",
    "mod = ???\n",
    "\n",
    "# fit the model\n",
    "res_altogether = mod.fit()\n",
    "\n",
    "# The coefficients\n",
    "res_altogether.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, everything fine... but let's scroll down until the end of the summary with note \\[2\\]. There is one word within this cryptic message that should raise all alarm bells: multicollinearity! Indeed our regressors are colinear. To see this, first reply this question: **in each trial, what is the sum of all 6 bin regressors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The sum is always one. This is because in each trial, the value of the binary variable is 1 for one and only one bin, and zeros for the other bins. So their sum is one.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collinearity (or multicollinearity) appears when a linear combination of a set of regressors is always null. **What linear combination of regressors is always null?** (Hint: remember there's a regressor of ones for the intercept)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we note $x_0=1$ the fixed regressor included for the intercept, then we find that $bin1 + bin2 + bin3 + bin4 + bin5 + bin6 - x_0 = 0$, for all trials.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our system of regressors is multicollinear, i.e. redundant, which means that we should not interpret the estimated weights (or p-values). **The weights of a regression model with colinear regressors are meaningless!** (since they cannot be uniquely determined)\n",
    "\n",
    "We are left with no other choice than removing one of the regressors. We will compare two methods for doing so.\n",
    "First, **fit the same model but without the regressor corresponding to the first bin of stimulus value**.\n",
    "\n",
    "Note: you can check that this gives exactly the same as declaring `x_bin` as a categorical regressor into the model using the formula with `C(x_bin)`. In this case, one-hot encoding is implicitly applied when defining the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>estim</td>      <th>  R-squared:         </th> <td>   0.033</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.033</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   98.69</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 22 Feb 2024</td> <th>  Prob (F-statistic):</th> <td>1.23e-102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:09:39</td>     <th>  Log-Likelihood:    </th> <td> -65698.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 14418</td>      <th>  AIC:               </th> <td>1.314e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 14412</td>      <th>  BIC:               </th> <td>1.315e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -5.9361</td> <td>    0.532</td> <td>  -11.166</td> <td> 0.000</td> <td>   -6.978</td> <td>   -4.894</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin2</th>      <td>    2.8565</td> <td>    0.752</td> <td>    3.800</td> <td> 0.000</td> <td>    1.383</td> <td>    4.330</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin3</th>      <td>    5.4373</td> <td>    0.635</td> <td>    8.556</td> <td> 0.000</td> <td>    4.192</td> <td>    6.683</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin4</th>      <td>    8.7819</td> <td>    0.704</td> <td>   12.482</td> <td> 0.000</td> <td>    7.403</td> <td>   10.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin5</th>      <td>   11.6014</td> <td>    0.752</td> <td>   15.435</td> <td> 0.000</td> <td>   10.128</td> <td>   13.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin6</th>      <td>   13.5100</td> <td>    0.752</td> <td>   17.969</td> <td> 0.000</td> <td>   12.036</td> <td>   14.984</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>5050.296</td> <th>  Durbin-Watson:     </th>  <td>   1.754</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>292783.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.878</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>25.006</td>  <th>  Cond. No.          </th>  <td>    7.81</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  estim   R-squared:                       0.033\n",
       "Model:                            OLS   Adj. R-squared:                  0.033\n",
       "Method:                 Least Squares   F-statistic:                     98.69\n",
       "Date:                Thu, 22 Feb 2024   Prob (F-statistic):          1.23e-102\n",
       "Time:                        12:09:39   Log-Likelihood:                -65698.\n",
       "No. Observations:               14418   AIC:                         1.314e+05\n",
       "Df Residuals:                   14412   BIC:                         1.315e+05\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -5.9361      0.532    -11.166      0.000      -6.978      -4.894\n",
       "bin2           2.8565      0.752      3.800      0.000       1.383       4.330\n",
       "bin3           5.4373      0.635      8.556      0.000       4.192       6.683\n",
       "bin4           8.7819      0.704     12.482      0.000       7.403      10.161\n",
       "bin5          11.6014      0.752     15.435      0.000      10.128      13.075\n",
       "bin6          13.5100      0.752     17.969      0.000      12.036      14.984\n",
       "==============================================================================\n",
       "Omnibus:                     5050.296   Durbin-Watson:                   1.754\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           292783.194\n",
       "Skew:                           0.878   Prob(JB):                         0.00\n",
       "Kurtosis:                      25.006   Cond. No.                         7.81\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model formula and database (point to a dataframe)\n",
    "mod_nobin1 = ???\n",
    "\n",
    "# fit the model\n",
    "res_nobin1 = mod_nobin1.fit()\n",
    "\n",
    "# The coefficients\n",
    "res_nobin1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility is to remove the fixed regressor that captures the intercept. This regressor is included automatically by default by `ols`, but we can choose not to include it by adding `+0` in the formula. For example, `estim ~ xavg + 0` fits a simple linear regression model without intercept. **Fit the model with all stimulus bin regressors but no intercept.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>estim</td>      <th>  R-squared:         </th> <td>   0.033</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.033</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   98.69</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 22 Feb 2024</td> <th>  Prob (F-statistic):</th> <td>1.23e-102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:09:39</td>     <th>  Log-Likelihood:    </th> <td> -65698.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 14418</td>      <th>  AIC:               </th> <td>1.314e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 14412</td>      <th>  BIC:               </th> <td>1.315e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin1</th> <td>   -5.9361</td> <td>    0.532</td> <td>  -11.166</td> <td> 0.000</td> <td>   -6.978</td> <td>   -4.894</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin2</th> <td>   -3.0796</td> <td>    0.531</td> <td>   -5.796</td> <td> 0.000</td> <td>   -4.121</td> <td>   -2.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin3</th> <td>   -0.4988</td> <td>    0.348</td> <td>   -1.433</td> <td> 0.152</td> <td>   -1.181</td> <td>    0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin4</th> <td>    2.8458</td> <td>    0.461</td> <td>    6.175</td> <td> 0.000</td> <td>    1.942</td> <td>    3.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin5</th> <td>    5.6653</td> <td>    0.531</td> <td>   10.662</td> <td> 0.000</td> <td>    4.624</td> <td>    6.707</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bin6</th> <td>    7.5739</td> <td>    0.532</td> <td>   14.247</td> <td> 0.000</td> <td>    6.532</td> <td>    8.616</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>5050.296</td> <th>  Durbin-Watson:     </th>  <td>   1.754</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>292783.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.878</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>25.006</td>  <th>  Cond. No.          </th>  <td>    1.53</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  estim   R-squared:                       0.033\n",
       "Model:                            OLS   Adj. R-squared:                  0.033\n",
       "Method:                 Least Squares   F-statistic:                     98.69\n",
       "Date:                Thu, 22 Feb 2024   Prob (F-statistic):          1.23e-102\n",
       "Time:                        12:09:39   Log-Likelihood:                -65698.\n",
       "No. Observations:               14418   AIC:                         1.314e+05\n",
       "Df Residuals:                   14412   BIC:                         1.315e+05\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "bin1          -5.9361      0.532    -11.166      0.000      -6.978      -4.894\n",
       "bin2          -3.0796      0.531     -5.796      0.000      -4.121      -2.038\n",
       "bin3          -0.4988      0.348     -1.433      0.152      -1.181       0.184\n",
       "bin4           2.8458      0.461      6.175      0.000       1.942       3.749\n",
       "bin5           5.6653      0.531     10.662      0.000       4.624       6.707\n",
       "bin6           7.5739      0.532     14.247      0.000       6.532       8.616\n",
       "==============================================================================\n",
       "Omnibus:                     5050.296   Durbin-Watson:                   1.754\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           292783.194\n",
       "Skew:                           0.878   Prob(JB):                         0.00\n",
       "Kurtosis:                      25.006   Cond. No.                         1.53\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model formula and database (point to a dataframe)\n",
    "mod_nointercept = ???\n",
    "\n",
    "# fit the model\n",
    "res_nointercept = mod_nointercept.fit()\n",
    "\n",
    "# The coefficients\n",
    "res_nointercept.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check that that dreadful \"Note \\[2\\]\" is absent from both of these models. \n",
    "Both models are valid and equivalent, but how do their weights compare? Let us **plot these weights in a bar plot** (we will do it by converting weight objects to dataframe and then merging the two dataframes using `join`). Note that the bars are absent when the regressor is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# converts weights for model without bin1 to dataframe\n",
    "df1 = pd.DataFrame({'NoBin1':??})\n",
    "\n",
    "# converts weights for model without intercept to dataframe\n",
    "df2 = pd.DataFrame({'NoIntercept':??})\n",
    "\n",
    "# join the two dataframes (missing values are filled with nans)\n",
    "df_weights = df1.join(df2,how = 'outer')\n",
    "                         \n",
    "# plot\n",
    "df_weights.plot.bar();\n",
    "plt.ylabel('weights');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do the weights of the two models differ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The value of the weights for bin1 in the no-intercept model is the same as the intercept in the no-bin1 model. For other bins, the weights differ by a constant term which corresponds to the intercept. In other words, the weight for binX in the no-intercept model is the sum of the corresponding weight and the intercept in the no-bin1 model. These two models are mathematically equivalent, but caution about how to interpret the bin weights: in one case, this is w.r.t to the weight of the reference value (no-bin1 model), while for the no-intercept model it is the raw value.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Checking for colinear or highly correlated regressors\n",
    "Because we won't always remember to look for a possible note at the bottom of a summary to check whether our model makes sense, it is good to **perform checks prior to fitting**. Moreover, while colinearity is a fatal issue for regression models (unless we introduce priors or regularizers over weights, but we don't cover these during our course), **strong correlations between regressors can also severely damage our ability to interpret regression results**. Basically, if a linear combination of regressors is not exactly null but close enough to zero for all trials, then regressors can be estimated, but with a very large uncertainty.\n",
    "This is why introducing too many regressors may lead to bad estimation through **overfitting**: as we add more and more regressors, we are bound to find *some* combination of regressors that is close enough to zero. This problem may be alleviated if we have a sufficiently large dataset.\n",
    "\n",
    "The good news is that we can check *a priori* whether regressors are colinear or highly correlated using the **Variance Inflation Factor** (VIF). The VIF is a metric for each regressor in a regression model that determines how much estimation will be affected by correlation with other regressors (technically, how much the variance of the weight is inflated by the presence of the other regressors). Its value is always larger than 1, and the smaller the better. It is 1 if the regressor under study is completely uncorrelated to other regressors. It is infinite when regressors are colinear, and very large if the regressor is strongly correlated to others. A rule of thumb is that *if any VIF is larger than 5 (sometimes we see 10), then the model cannot be properly estimated and one regressor must be excluded.* But this is just a rule of thumb, how severe the problem is depends on the actual dataset. In any case, large VIFs should at minimum raise caution. \n",
    "VIFs can be computed using `variance_inflation_factor` from the `statsmodel` package.\n",
    "**Check the VIF for the full set of stimulus bins as regressors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF for bin  1 :  inf\n",
      "VIF for bin  2 :  inf\n",
      "VIF for bin  3 :  inf\n",
      "VIF for bin  4 :  inf\n",
      "VIF for bin  5 :  inf\n",
      "VIF for bin  6 :  inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:195: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "\n",
    "# define the set of regressors we want to test independent variables set \n",
    "X = df[['bin1', 'bin2','bin3','bin4','bin5','bin6']]\n",
    "\n",
    "# convert to array\n",
    "X = X.values\n",
    "\n",
    "# remember to add fixed regressor (for intercept)\n",
    "fixed_regressor = ??? # 2D array with same number of lines as X and one column\n",
    "X = np.concatenate((??,??),axis =1 )\n",
    "\n",
    "vif= []\n",
    "for i in range(nBin): # loop through bins\n",
    "    # compute VIF for corresponding regressor\n",
    "    V = variance_inflation_factor(X, i)\n",
    "    vif.append(V)\n",
    "\n",
    "    print(\"VIF for bin \", i+1, \": \", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that indeed VIF values are infinite, which denotes multicollinearity.\n",
    "**Compute VIF values for the same set of regressors, excluding regressor for the first bin.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF for bin  2 :  1.7397231250528662\n",
      "VIF for bin  3 :  2.318352133670704\n",
      "VIF for bin  4 :  1.926064272692391\n",
      "VIF for bin  5 :  1.7397231250528662\n",
      "VIF for bin  6 :  1.7390761548064917\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "\n",
    "# define the set of regressors we want to test independent variables set \n",
    "df_sub = df[['bin2','bin3','bin4','bin5','bin6']]\n",
    "\n",
    "# convert to array\n",
    "X = df_sub.values\n",
    "\n",
    "# remember to add fixed regressor (for intercept)\n",
    "????\n",
    "\n",
    "vif= []\n",
    "for i in range(??): # loop through bins\n",
    "    # compute VIF for corresponding regressor\n",
    "    V = variance_inflation_factor(X, i)\n",
    "    vif.append(V)\n",
    "\n",
    "    print(\"VIF for bin \", i+2, \": \", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you conclude about this set of regressors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The values of VIF are close to 2, which is clearly on the safe side (lower than 5). This shows that the model with this set of regressors is well defined.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below allows to plot VIF values for variables in a dataframe (the intercept is added within the function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vif(df):\n",
    "\n",
    "    # VIF dataframe \n",
    "    VIF_df = pd.DataFrame() \n",
    "    VIF_df[\"regressor\"] = df.columns \n",
    "\n",
    "    # convert to array\n",
    "    X = df.values\n",
    "    nReg = X.shape[1] # number of regressors\n",
    "\n",
    "    # add fixed regressor (for intercept)\n",
    "    fixed_regressor = np.ones((X.shape[0],1))\n",
    "    X = np.concatenate((X,fixed_regressor),axis =1 )\n",
    "\n",
    "    # calculating VIF for each feature \n",
    "    VIF_df[\"VIF\"] = [variance_inflation_factor(X, i) \n",
    "                            for i in range(nReg)] \n",
    "\n",
    "    VIF_df.plot.barh(x='regressor',y='VIF');\n",
    "    plt.xlabel('Variance Inflation Factor');\n",
    "    plt.plot((1,1),(-1,nReg),'r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAAGwCAYAAABGogSnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwJElEQVR4nO3deXgUdZ7H8U+ThEDnlAAGlpYICSGi3CIBFVhEHNCNMC6YjQKSNSgmEIIHKKDoCDgYE8ARHMPpeqByPIzuw+FwiHIKQQZhMYMcGQ4jVxpCTEhS+wdDj20SIE0n3aHer+ep5+mq+tUv3+qinv5Q9esui2EYhgAAAEyijqcLAAAAqEmEHwAAYCqEHwAAYCqEHwAAYCqEHwAAYCqEHwAAYCqEHwAAYCq+ni7AG5WVlenYsWMKCgqSxWLxdDkAAOAaGIahc+fOqWnTpqpTp/LrO4SfChw7dkw2m83TZQAAABfk5uaqWbNmla4n/FQgKChI0qU3Lzg42MPVAF6qoEBq2vTS62PHpIAAz9YDwPTsdrtsNpvjc7wyhJ8KXL7VFRwcTPgBKuPj86/XwcGEHwBe42pDVhjwDAAATIXwAwAATIXwAwAATIUxPwAAeKnS0lJdvHjR02V4DT8/P/n8eryhiwg/AAB4GcMwdOLECZ09e9bTpXid0NBQhYeHX9fv8BF+AADwMpeDT+PGjWW1WvnBXV0KhBcuXFBeXp4kqUmTJi73RfgBAMCLlJaWOoJPWFiYp8vxKvXr15ck5eXlqXHjxi7fAmPAMwAAXuTyGB+r1erhSrzT5fflesZCEX4AAPBC3OqqmDveF8IPAAAwFcIPAAAwFQY8AwBQS0SM+6JG/96haf1r9O/VFK78AACA6/bQQw/pvvvuq3Dd5s2bZbFYtHPnTlksFu3atUuSdOjQIVkslnLTY489Vq21cuUHAABct8TERA0cOFCHDx9W8+bNndbNmzdP7du3V4MGDSrc9ssvv1SbNm0c85e/0l5duPIDAACu24MPPqjGjRtrwYIFTssvXLigxYsXKzExsdJtw8LCFB4e7phCQkKqtVbCDwAAuG6+vr4aMmSIFixYIMMwHMs//fRTFRcXKyEhwYPVOeO21xXc/vIq1fHnR6aAitQv/kX7/vk6ZuJKFdat59F6cOO5UQfb3siGDx+u6dOna/369erVq5ekS7e8Bg4cqJtuukn5+fkVbtetWzfVqfOv6zEbN25Uhw4dqq1Owg8AAHCL1q1bq1u3bpo3b5569eqlAwcOaOPGjVq9evUVt1u8eLFiYmIc8zabrVrr5LYXAABwm8TERC1ZskR2u13z589X8+bN1bt37ytuY7PZFBkZ6Zj8/f2rtUbCDwAAcJtBgwbJx8dHH374oRYuXKgnnnjC6x7VwW0vAADgNoGBgRo8eLBefPFF5efna9iwYZ4uqRzCDwAAtURtGQSemJiouXPn6v7779ctt9zi6XLKIfwAAAC3io2Ndfq6+2URERFOy387X1MY8wMAAEyF8AMAAEyF8AMAAEyF8AMAgBfyxFiY2sAd7wvhBwAAL+Ln5yfp0gNBUd7l9+Xy++QKvu0FAIAX8fHxUWhoqPLy8iRJVqvV634k0BMMw9CFCxeUl5en0NBQ+fj4uNyXR8NPz5491b59e2VmZla4PiIiQqmpqUpNTa3RugAA8KTw8HBJcgQg/EtoaKjj/XGVV1/52b59uwICAqq83YIFC/TWW2/phx9+UGhoqB555BG9/fbb1VAhAADuZ7FY1KRJEzVu3FgXL170dDlew8/P77qu+Fzm1eGnUaNGVd7mrbfeUnp6uqZPn6677rpLv/zyi3788cdqqA4AgOrl4+Pjlg97OPP4gOeSkhIlJycrNDRUYWFhmjBhgmMkd0REhNMtMYvFoqysLA0YMEBWq1VRUVFasWKFY/2ZM2c0YcIELVq0SP/1X/+lli1bqk2bNnrooYdqercAAICX8nj4WbhwoXx9fbV161bNnDlTGRkZysrKqrT95MmTNWjQIO3evVv9+vVTQkKCTp8+LUlas2aNysrKdPToUcXExKhZs2YaNGiQcnNzr1hDUVGR7Ha70wQAAG5MHg8/NptNGRkZio6OVkJCglJSUpSRkVFp+2HDhik+Pl6RkZGaMmWKCgoKtG3bNknSjz/+qLKyMk2ZMkWZmZn67LPPdPr0afXp00fFxcWV9jl16lSFhIQ4JpvN5vb9BAAA3sHj4adr165OX+GLjY1VTk6OSktLK2zftm1bx+uAgAAFBQU5RsOXlZXp4sWLmjlzpvr27auuXbvqo48+Uk5OjtatW1dpDePHj1d+fr5jutqVIgAAUHt59YDnivz2R40sFovKysokSU2aNJEk3XbbbY71jRo1UsOGDXXkyJFK+/T395e/v381VAsAALyNx6/8bNmypdx8VFSUS6Pbu3fvLknav3+/Y9np06d18uRJNW/e/PoKBQAANwSPh5/c3FylpaVp//79+uijjzRr1iyNHj3apb5atWqluLg4jR49Wps2bdKePXs0dOhQtW7dWr169XJz5QAAoDby+G2vIUOGqLCwUF26dJGPj49SUlKUlJTkcn+LFi3SmDFj1L9/f9WpU0c9evTQypUrr+sZIAAA4MZhMXhsbDl2u/3St75SP1Edf6unywG8Uv3iX7Qv4xFJUsyYz1RYt56HK8KN5tC0/p4uAbXM5c/v/Px8BQcHV9rO47e9AAAAahLhBwAAmArhBwAAmArhBwAAmArhBwAAmArhBwAAmArhBwAAmArhBwAAmArhBwAAmArhBwAAmArhBwAAmArhBwAAmAoPNq3AtT4YDTC1ggIpMPDS6/PnpYAAz9YDwPR4sCkAAEAFCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUfD1dgDe7/eVVquNv9XQZgFeqX/yL9v3zdczElSqsW8+j9eDGc2haf0+XgBsUV34AAICpEH4AAICpEH4AAICpEH4AAICpEH4AAICpEH4AAICpEH4AAICpEH4AAICpEH4AAICpEH4AAICpEH4AAICpEH4AAICpeDT89OzZU6mpqZWuj4iIUGZmZo3VAwAAbnxefeVn+/btSkpKqtI2Foul3DRnzpxqqhAAANQ2vp4u4EoaNWrk0nbz58/XAw884JgPCQlxV0kAAKCW8/iVn5KSEiUnJys0NFRhYWGaMGGCDMOQVP62l8ViUVZWlgYMGCCr1aqoqCitWLGiXJ+hoaEKDw93TPXr16+p3QEAAF7O4+Fn4cKF8vX11datWzVz5kxlZGQoKyur0vaTJ0/WoEGDtHv3bvXr108JCQk6ffq0U5vk5GQ1bNhQd955p+bMmaOysrIr1lBUVCS73e40AQCAG5PHw4/NZlNGRoaio6OVkJCglJQUZWRkVNp+2LBhio+PV2RkpKZMmaKCggJt27bNsf61117Tp59+qi+//FKPPvqoxo4dqylTplyxhqlTpyokJMQx2Ww2t+0fAADwLh4f89O1a1dZLBbHfGxsrNLT01VaWlph+7Zt2zpeBwQEKCgoSHl5eY5lEyZMcLxu3769JOnVV191Wv5b48ePV1pammPebrcTgAAAuEF5PPxUlZ+fn9O8xWK54m2trl27ym6366efftLNN99cYRt/f3/5+/u7tU4AAOCdPH7ba8uWLeXmo6Ki5OPj45b+s7OzVa9ePYWGhrqlPwAAULt5/MpPbm6u0tLSNGLECO3cuVOzZs1Senq6S3395S9/0YkTJxQbG6v69etr3bp1eumll5SUlMSVHQAAIMkLws+QIUNUWFioLl26yMfHRykpKVX+YcPL/Pz89M477ygtLU1lZWVq0aKFXn31VT3zzDNurhoAANRWHg0/69evd7yePXt2ufWHDh1ymr/8+z+/dvbsWcfrBx54wOnHDQEAAH7L42N+AAAAahLhBwAAmArhBwAAmArhBwAAmArhBwAAmArhBwAAmArhBwAAmArhBwAAmArhBwAAmArhBwAAmArhBwAAmIrFqOiBWSZnt9sVEhKi/Px8BQcHe7ocwDsVFEiBgZdenz8vBQR4th4Apnetn99c+QEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZC+AEAAKZS5fBTUlKihQsX6sSJE9VRDwAAQLWqcvjx9fXV008/raKiouqoBwAAoFq5dNvrrrvu0q5du9xcCgAAQPXzdWWjkSNHKi0tTbm5uerUqZMCAgKc1rdt29YtxQEAALibxTAMo6ob1alT/oKRxWKRYRiyWCwqLS11S3GeYrfbFRISovz8fAUHB3u6HMA7FRRIgYGXXp8/L/3mP0EAUNOu9fPbpSs/Bw8edLkwAAAAT3Ip/DRv3tzddQAAANQIl8KPJB04cECZmZnat2+fLBaLYmJiNHr0aLVs2dKd9QEAALiVS9/2WrVqlW677TZt27ZNbdu21e23366tW7eqTZs2WrNmjbtrBAAAcBuXBjx36NBBffv21bRp05yWjxs3TqtXr9bOnTvdVqAnMOAZuAYMeAbgZa7189ulKz/79u1TYmJiueXDhw/X3r17XekSAACgRrg05qdRo0batWuXoqKinJbv2rVLjRs3dkth3uD2l1epjr/V02UAXql+8S/a98/XMRNXqrBuPY/WA9SUQ9P6e7oEXCeXws+TTz6ppKQk/fjjj+rWrZssFou+/vprvfHGGxo7dqy7awQAAHAbl8LPxIkTFRQUpPT0dI0fP16S1LRpU73yyisaNWqUWwsEAABwJ5fCj8Vi0ZgxYzRmzBidO3dOkhQUFOTWwgAAAKqDSwOeCwsLdeHCBUmXQs/p06eVmZmp1atXu7U4AAAAd3Mp/MTFxWnRokWSpLNnz6pLly5KT09XXFycZs+e7dYCAQAA3Mml8LNz507dc889kqTPPvtM4eHhOnz4sBYtWqSZM2e6tUAAAAB3cin8XLhwwTHGZ/Xq1Ro4cKDq1Kmjrl276vDhw24tEAAAwJ1cCj+RkZFavny5cnNztWrVKt1///2SpLy8PH4RGQAAeDWXws+kSZP07LPPKiIiQnfddZdiY2MlXboK1KFDB7cWCAAA4E4ufdX9kUce0d13363jx4+rXbt2juW9e/fWgAED3FYcAACAu7kUfiQpPDxc4eHhki49SGzt2rWKjo5W69at3VYcAACAu7l022vQoEF6++23JV36zZ/OnTtr0KBBatu2rZYsWeLWAgEAANzJpfDz1VdfOb7qvmzZMhmGobNnz2rmzJn6wx/+4NYCAQAA3Mml8JOfn68GDRpIklauXKnf//73slqt6t+/v3Jycq65n549eyo1NbXS9REREcrMzHSlRAAAgAq5NObHZrNp8+bNatCggVauXKmPP/5YknTmzBnVq1fPbcVt375dAQEBLm176tQptWvXTkePHtWZM2cUGhrqtroAAEDt5dKVn9TUVCUkJKhZs2Zq0qSJevbsKenS7bA77rjDbcU1atRIVqvVpW0TExPVtm1bt9UCAABuDC6Fn5EjR2rz5s2aN2+evvnmG9Wpc6mbFi1aVHnMT0lJiZKTkxUaGqqwsDBNmDBBhmFIKn/by2KxKCsrSwMGDJDValVUVJRWrFhRrs/Zs2fr7NmzevbZZ13ZPQAAcANzKfxIUufOndW/f38dPXpUJSUlkqT+/fure/fuVepn4cKF8vX11datWzVz5kxlZGQoKyur0vaTJ0/WoEGDtHv3bvXr108JCQk6ffq0Y/3evXv16quvatGiRY5QdjVFRUWy2+1OEwAAuDG5/GyvxMREWa1WtWnTRkeOHJEkjRo1StOmTatSXzabTRkZGYqOjlZCQoJSUlKUkZFRafthw4YpPj5ekZGRmjJligoKCrRt2zZJl0JMfHy8pk+frltuueWaa5g6dapCQkIck81mq9I+AACA2sOl8DN+/Hh99913Wr9+vdMA5/vuu0+LFy+uUl9du3aVxWJxzMfGxionJ0elpaUVtv/1OJ6AgAAFBQUpLy/PUVdMTIwee+yxKtUwfvx45efnO6bc3NwqbQ8AAGoPl8LP8uXL9fbbb+vuu+92Ci633XabDhw44LbiKuLn5+c0b7FYVFZWJklau3atPv30U/n6+srX11e9e/eWJDVs2FAvv/xypX36+/srODjYaQIAADcml77q/vPPP6tx48bllhcUFDiFoWuxZcuWcvNRUVHy8fGpcl1LlixRYWGhY3779u0aPny4Nm7cqJYtW1a5PwAAcONxKfzceeed+uKLL5SSkiJJjsDz3nvvOZ7wfq1yc3OVlpamESNGaOfOnZo1a5bS09NdKatcwDl58qQkKSYmht/5AQAAklwMP1OnTtUDDzygvXv3qqSkRDNmzND333+vzZs3a8OGDVXqa8iQISosLFSXLl3k4+OjlJQUJSUluVIWAADAVbkUfrp166ZNmzZp+vTpatmypVavXq2OHTtq8+bNVfqRw/Xr1ztez549u9z6Q4cOOc1f/v2fXzt79myl/ffs2bPCbQAAgHlVOfxcvHhRSUlJmjhxohYuXFgdNQEAAFSbKn/by8/PT8uWLauOWgAAAKqdS191HzBggJYvX+7mUgAAAKqfS2N+IiMj9dprr2nTpk3q1KlTuSevjxo1yi3FAQAAuJtL4ScrK0uhoaHasWOHduzY4bTOYrEQfgAAgNdyKfwcPHjQ3XUAAADUCJef6g4AAFAbuXTlJy0trcLlFotF9erVU2RkpOLi4tSgQYPrKg4AAMDdXAo/2dnZ2rlzp0pLSxUdHS3DMJSTkyMfHx+1bt1a77zzjsaOHauvv/5at912m7trBgAAcJlLt73i4uJ033336dixY9qxY4d27typo0ePqk+fPoqPj9fRo0d17733asyYMe6uFwAA4Lq4FH6mT5+u1157TcHBwY5lwcHBeuWVV/THP/5RVqtVkyZNKvdNMAAAAE9z6bZXfn6+8vLyyt3S+vnnn2W32yVJoaGhKi4uvv4KPWjP5L5OAQ/ArxQUSBmXXu577QHpN7/3BQDeyuXbXsOHD9eyZcv0j3/8Q0ePHtWyZcuUmJiohx9+WJK0bds2tWrVyp21AgAAXDeXrvy8++67GjNmjB599FGVlJRc6sjXV0OHDlVGxqX/CrZu3VpZWVnuqxQAAMANLIZhGK5ufP78ef34448yDEMtW7ZUYGCgO2vzGLvdrpCQEOXn53PbC6hMQYF0+Zw/f57bXgA87lo/v6/rRw5PnDih48ePq1WrVgoMDNR15CgAAIAa4VL4OXXqlHr37q1WrVqpX79+On78uCTpv//7vzV27Fi3FggAAOBOLoWfMWPGyM/PT0eOHJHVanUsHzx4sFauXOm24gAAANzNpQHPq1ev1qpVq9SsWTOn5VFRUTp8+LBbCgMAAKgOLl35KSgocLric9nJkyfl7+9/3UUBAABUF5fCz7333qtFixY55i0Wi8rKyjR9+nT16tXLbcUBAAC4m0u3vd5880316NFD3377rYqLi/X888/r+++/1+nTp/XNN9+4u0YAAAC3qfKVn4sXL2rkyJFasWKFunTpoj59+qigoEADBw5Udna2WrZsWR11AgAAuEWVr/z4+flpz549CgsL0+TJk6ujJgAAgGrj0pifIUOGaO7cue6uBQAAoNq5NOanuLhYWVlZWrNmjTp37qyA3/ys/VtvveWW4gAAANzNpfCzZ88edezYUZL0ww8/OK2zWCzXXxUAAEA1cSn8rFu3zt11AAAA1IjrerApAABAbUP4AQAApkL4AQAApkL4AQAApkL4AQAApkL4AQAApkL4AQAApkL4AQAApkL4AQAApkL4AQAApkL4AQAApkL4AQAApkL4AQAApkL4AQAApkL4AQAApkL4AQAApkL4AQAApkL4AQAApkL4AQAApkL4AQAApuLr6QK82e0vr1Idf6unywC8Uv3iX7Tvn69jJq5UYd16Hq0HQO1waFp/T5fAlR8AAGAuhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqHg0/PXv2VGpqaqXrIyIilJmZWWP1AACAG59XX/nZvn27kpKSrrn9qVOn9MADD6hp06by9/eXzWZTcnKy7HZ7NVYJAABqE68OP40aNZLVar3m9nXq1FFcXJxWrFihH374QQsWLNCXX36pp556qhqrBAAAtYnHw09JSYmSk5MVGhqqsLAwTZgwQYZhSCp/28tisSgrK0sDBgyQ1WpVVFSUVqxY4Vh/00036emnn1bnzp3VvHlz9e7dWyNHjtTGjRtrercAAICX8nj4WbhwoXx9fbV161bNnDlTGRkZysrKqrT95MmTNWjQIO3evVv9+vVTQkKCTp8+XWHbY8eOaenSperRo8cVaygqKpLdbneaAADAjcnj4cdmsykjI0PR0dFKSEhQSkqKMjIyKm0/bNgwxcfHKzIyUlOmTFFBQYG2bdvm1CY+Pl5Wq1X/9m//puDg4CuGKUmaOnWqQkJCHJPNZnPLvgEAAO/j8fDTtWtXWSwWx3xsbKxycnJUWlpaYfu2bds6XgcEBCgoKEh5eXlObTIyMrRz504tX75cBw4cUFpa2hVrGD9+vPLz8x1Tbm7udewRAADwZr6eLqCq/Pz8nOYtFovKysqcloWHhys8PFytW7dWWFiY7rnnHk2cOFFNmjSpsE9/f3/5+/tXW80AAMB7ePzKz5YtW8rNR0VFycfHxy39Xx48XVRU5Jb+AABA7ebxKz+5ublKS0vTiBEjtHPnTs2aNUvp6eku9fW///u/+umnn3TnnXcqMDBQe/fu1fPPP6/u3bsrIiLCvYUDAIBayePhZ8iQISosLFSXLl3k4+OjlJSUKv2w4a/Vr19f7733nsaMGaOioiLZbDYNHDhQ48aNc3PVAACgtvJo+Fm/fr3j9ezZs8utP3TokNP85VtYv3b27FnH6169emnTpk3uKg8AANyAPD7mBwAAoCYRfgAAgKkQfgAAgKkQfgAAgKkQfgAAgKkQfgAAgKkQfgAAgKkQfgAAgKkQfgAAgKkQfgAAgKkQfgAAgKlYjIoemGVydrtdISEhys/PV3BwsKfLAbxTQYEUGHjp9fnzUkCAZ+sBYHrX+vnNlR8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqvp4uwJvd/vIq1fG3eroMwCvVL/5F+/75OmbiShXWrefRenDjOTStv6dLwA2KKz8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUCD8AAMBUPBp+evbsqdTU1ErXR0REKDMzs8bqAQAANz6vvvKzfft2JSUlXXP77777TvHx8bLZbKpfv75iYmI0Y8aMaqwQAADUNr6eLuBKGjVqVKX2O3bsUKNGjfQ///M/stls2rRpk5KSkuTj46Pk5ORqqhIAANQmHr/yU1JSouTkZIWGhiosLEwTJkyQYRiSyt/2slgsysrK0oABA2S1WhUVFaUVK1Y41g8fPlwzZ85Ujx491KJFCz322GN64okntHTp0preLQAA4KU8Hn4WLlwoX19fbd26VTNnzlRGRoaysrIqbT958mQNGjRIu3fvVr9+/ZSQkKDTp09X2j4/P18NGjS4Yg1FRUWy2+1OEwAAuDF5PPzYbDZlZGQoOjpaCQkJSklJUUZGRqXthw0bpvj4eEVGRmrKlCkqKCjQtm3bKmy7efNmffLJJxoxYsQVa5g6dapCQkIck81mu659AgAA3svj4adr166yWCyO+djYWOXk5Ki0tLTC9m3btnW8DggIUFBQkPLy8sq1+/777xUXF6dJkyapT58+V6xh/Pjxys/Pd0y5ubku7g0AAPB2Xj3guSJ+fn5O8xaLRWVlZU7L9u7dq3//93/Xk08+qQkTJly1T39/f/n7+7u1TgAA4J08fuVny5Yt5eajoqLk4+PjUn/ff/+9evXqpaFDh+r11193R4kAAOAG4vHwk5ubq7S0NO3fv18fffSRZs2apdGjR7vU1+Xg06dPH6WlpenEiRM6ceKEfv75ZzdXDQAAaiuP3/YaMmSICgsL1aVLF/n4+CglJaVKP2z4a59++ql+/vlnffDBB/rggw8cy5s3b65Dhw65qWIAAFCbWYzLP6oDB7vdfulbX6mfqI6/1dPlAF6pfvEv2pfxiCQpZsxnKqxbz8MV4UZzaFp/T5eAWuby53d+fr6Cg4Mrbefx214AAAA1ifADAABMhfADAABMhfADAABMhfADAABMhfADAABMhfADAABMhfADAABMhfADAABMhfADAABMhfADAABMhWd7VeBanw0CmFpBgRQYeOn1+fNSQIBn6wFgejzbCwAAoAKEHwAAYCqEHwAAYCqEHwAAYCqEHwAAYCqEHwAAYCqEHwAAYCqEHwAAYCqEHwAAYCqEHwAAYCqEHwAAYCqEHwAAYCqEHwAAYCqEHwAAYCqEHwAAYCqEHwAAYCqEHwAAYCq+ni7AGxmGIUmy2+0ergTwYgUF/3ptt0ulpZ6rBQD0r8/ty5/jlSH8VODUqVOSJJvN5uFKgFqiaVNPVwAADufOnVNISEil6wk/FWjQoIEk6ciRI1d881Cz7Ha7bDabcnNzFRwc7OlyII6Jt+K4eCeOS/UzDEPnzp1T06v8h4zwU4E6dS4NhQoJCeEfqBcKDg7muHgZjol34rh4J45L9bqWixYMeAYAAKZC+AEAAKZC+KmAv7+/Xn75Zfn7+3u6FPwKx8X7cEy8E8fFO3FcvIfFuNr3wQAAAG4gXPkBAACmQvgBAACmQvgBAACmQvgBAACmYsrw88477+jWW29VvXr11KlTJ23cuPGK7Tds2KBOnTqpXr16atGihebMmVNDlZpLVY7L+vXrZbFYyk3/93//V4MV3/i++uorPfTQQ2ratKksFouWL19+1W04X6pfVY8L50v1mzp1qu68804FBQWpcePGevjhh7V///6rbsf54hmmCz+LFy9WamqqXnrpJWVnZ+uee+7R7373Ox05cqTC9gcPHlS/fv10zz33KDs7Wy+++KJGjRqlJUuW1HDlN7aqHpfL9u/fr+PHjzumqKioGqrYHAoKCtSuXTu9/fbb19Se86VmVPW4XMb5Un02bNigZ555Rlu2bNGaNWtUUlKi+++/XwW/fgDwb3C+eJBhMl26dDGeeuopp2WtW7c2xo0bV2H7559/3mjdurXTshEjRhhdu3atthrNqKrHZd26dYYk48yZMzVQHQzDMCQZy5Ytu2Ibzpeady3HhfOl5uXl5RmSjA0bNlTahvPFc0x15ae4uFg7duzQ/fff77T8/vvv16ZNmyrcZvPmzeXa9+3bV99++60uXrxYbbWaiSvH5bIOHTqoSZMm6t27t9atW1edZeIacL54N86XmpOfny/pXw/Krgjni+eYKvycPHlSpaWluvnmm52W33zzzTpx4kSF25w4caLC9iUlJTp58mS11WomrhyXJk2a6M9//rOWLFmipUuXKjo6Wr1799ZXX31VEyWjEpwv3onzpWYZhqG0tDTdfffduv322yttx/niOaZ8qrvFYnGaNwyj3LKrta9oOa5PVY5LdHS0oqOjHfOxsbHKzc3Vm2++qXvvvbda68SVcb54H86XmpWcnKzdu3fr66+/vmpbzhfPMNWVn4YNG8rHx6fc1YS8vLxy6fuy8PDwCtv7+voqLCys2mo1E1eOS0W6du2qnJwcd5eHKuB8qT04X6pHSkqKVqxYoXXr1qlZs2ZXbMv54jmmCj9169ZVp06dtGbNGqfla9asUbdu3SrcJjY2tlz71atXq3PnzvLz86u2Ws3EleNSkezsbDVp0sTd5aEKOF9qD84X9zIMQ8nJyVq6dKnWrl2rW2+99arbcL54kAcHW3vExx9/bPj5+Rlz58419u7da6SmphoBAQHGoUOHDMMwjHHjxhmPP/64o/2PP/5oWK1WY8yYMcbevXuNuXPnGn5+fsZnn33mqV24IVX1uGRkZBjLli0zfvjhB2PPnj3GuHHjDEnGkiVLPLULN6Rz584Z2dnZRnZ2tiHJeOutt4zs7Gzj8OHDhmFwvnhKVY8L50v1e/rpp42QkBBj/fr1xvHjxx3ThQsXHG04X7yH6cKPYRjGn/70J6N58+ZG3bp1jY4dOzp9FXHo0KFGjx49nNqvX7/e6NChg1G3bl0jIiLCmD17dg1XbA5VOS5vvPGG0bJlS6NevXrGTTfdZNx9993GF1984YGqb2yXvyL922no0KGGYXC+eEpVjwvnS/Wr6HhIMubPn+9ow/niPSyG8c/RVQAAACZgqjE/AAAAhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB8AAGAqhB/A5CwWi5YvX+7pMqrFn//8Z9lsNtWpU0eZmZl65ZVX1L59++vu1139APAMwg/g5R566CHdd999Fa7bvHmzLBaLdu7c6XL/x48f1+9+9zuXt68Jhw4dksVi0a5du655G7vdruTkZL3wwgs6evSokpKSXPrbFYXDZ599Vn/9619d6q8qIiIiZLFYnKarPSn8Wt3IoRe4GsIP4OUSExO1du1aHT58uNy6efPmqX379urYsWOV+y0uLpYkhYeHy9/f/7rr9DZHjhzRxYsX1b9/fzVp0kRWq9VtfQcGBiosLMxt/V3Jq6++quPHjzum7OzsGvm71+rixYueLgGoMsIP4OUefPBBNW7cWAsWLHBafuHCBS1evFiJiYk6deqU4uPj1axZM1mtVt1xxx366KOPnNr37NlTycnJSktLU8OGDdWnTx9J5a8AvPDCC2rVqpWsVqtatGihiRMnOn3AXb7l8/777ysiIkIhISF69NFHde7cOUebsrIyvfHGG4qMjJS/v79uueUWvf766471R48e1eDBg3XTTTcpLCxMcXFxOnTo0DW/J+vXr5fFYtFf//pXde7cWVarVd26ddP+/fslSQsWLNAdd9whSWrRooUsFkuF/W/fvl19+vRRw4YNFRISoh49ejhdRYuIiJAkDRgwQBaLxTH/29teZWVlevXVV9WsWTP5+/urffv2WrlypWP95StXS5cuVa9evWS1WtWuXTtt3rz5qvsaFBSk8PBwx9SoUSOVlpYqMTFRt956q+rXr6/o6GjNmDGj3Lbz5s1TmzZt5O/vryZNmig5OfmK+yVJs2fPVsuWLVW3bl1FR0fr/fffd+rTYrFozpw5iouLU0BAgP7whz9cdR8Ab0P4Abycr6+vhgwZogULFujXzyH+9NNPVVxcrISEBP3yyy/q1KmTPv/8c+3Zs0dJSUl6/PHHtXXrVqe+Fi5cKF9fX33zzTd69913K/x7QUFBWrBggfbu3asZM2bovffeU0ZGhlObAwcOaPny5fr888/1+eefa8OGDZo2bZpj/fjx4/XGG29o4sSJ2rt3rz788EPdfPPNki6Ftl69eikwMFBfffWVvv76awUGBuqBBx5wXI26Vi+99JLS09P17bffytfXV8OHD5ckDR48WF9++aUkadu2bTp+/LhsNlu57c+dO6ehQ4dq48aN2rJli6KiotSvXz9HkNu+fbskaf78+Tp+/Lhj/rdmzJih9PR0vfnmm9q9e7f69u2r//iP/1BOTk65ep999lnt2rVLrVq1Unx8vEpKSqq0z9KlsNWsWTN98skn2rt3ryZNmqQXX3xRn3zyiaPN7Nmz9cwzzygpKUl/+9vftGLFCkVGRl5xv5YtW6bRo0dr7Nix2rNnj0aMGKEnnnhC69atc/r7L7/8suLi4vS3v/3N8Z4DtYqHnyoP4Brs27fPkGSsXbvWsezee+814uPjK92mX79+xtixYx3zPXr0MNq3b1+unSRj2bJllfbzxz/+0ejUqZNj/uWXXzasVqtht9sdy5577jnjrrvuMgzDMOx2u+Hv72+89957FfY3d+5cIzo62igrK3MsKyoqMurXr2+sWrWqwm0OHjxoSDKys7MNwzCMdevWGZKML7/80tHmiy++MCQZhYWFhmEYRnZ2tiHJOHjwoFPt7dq1q3RfS0pKjKCgIOMvf/mLY1lF789v+2natKnx+uuvO7W58847jZEjRzrVn5WV5Vj//fffG5KMffv2VVpP8+bNjbp16xoBAQGOacaMGRW2HTlypPH73//eqaaXXnqp0r4r2q9u3boZTz75pNOy//zP/zT69evntF1qamql/QK1ga+HMheAKmjdurW6deumefPmqVevXjpw4IA2btyo1atXS5JKS0s1bdo0LV68WEePHlVRUZGKiooUEBDg1E/nzp2v+rc+++wzZWZm6u9//7vOnz+vkpISBQcHO7WJiIhQUFCQY75JkybKy8uTJO3bt09FRUXq3bt3hf3v2LFDf//73522l6RffvlFBw4cuPqb8Stt27Z1qkGS8vLydMstt1zT9nl5eZo0aZLWrl2rn376SaWlpbpw4YKOHDlyzTXY7XYdO3ZM3bt3d1revXt3fffdd9dUb+vWrSvt/7nnntOwYcMc8w0bNpQkzZkzR1lZWTp8+LAKCwtVXFzsuBWXl5enY8eOVXoMKrNv375yA8O7d+9e7pbatfw7ArwZ4QeoJRITE5WcnKw//elPmj9/vpo3b+74cEtPT1dGRoYyMzN1xx13KCAgQKmpqeVuI/02DP3Wli1b9Oijj2ry5Mnq27evQkJC9PHHHys9Pd2pnZ+fn9O8xWJRWVmZJKl+/fpX/BtlZWXq1KmTPvjgg3LrGjVqdMVtf+vXdVgsFkf/12rYsGH6+eeflZmZqebNm8vf31+xsbFVvv32679/mWEY5Za5Um/Dhg0dt6su++STTzRmzBilp6crNjZWQUFBmj59uuM259WOwfXux9X+HQHejjE/QC0xaNAg+fj46MMPP9TChQv1xBNPOD6UNm7cqLi4OD322GNq166dWrRoUW68ybX45ptv1Lx5c7300kvq3LmzoqKiKvyW2ZVERUWpfv36lX4VvGPHjsrJyVHjxo0VGRnpNIWEhFS55uuxceNGjRo1Sv369XMMDD558qRTGz8/P5WWllbaR3BwsJo2baqvv/7aafmmTZsUExNTbXV369ZNI0eOVIcOHRQZGel01SwoKEgRERFX/Dp+RfsVExNTo/sBeArhB6glAgMDNXjwYL344os6duyY062QyMhIrVmzRps2bdK+ffs0YsQInThxosp/IzIyUkeOHNHHH3+sAwcOaObMmVq2bFmV+qhXr55eeOEFPf/881q0aJEOHDigLVu2aO7cuZKkhIQENWzYUHFxcdq4caMOHjyoDRs2aPTo0frHP/5R5ZqvR2RkpN5//33t27dPW7duVUJCQrmrJpdDxIkTJ3TmzJkK+3nuuef0xhtvaPHixdq/f7/GjRunXbt2afTo0dVW97fffqtVq1bphx9+0MSJE8sNxn7llVeUnp6umTNnKicnRzt37tSsWbOuuF/PPfecFixYoDlz5ignJ0dvvfWWli5dqmeffbZa9gPwFMIPUIskJibqzJkzuu+++5zGtUycOFEdO3ZU37591bNnT4WHh+vhhx+ucv9xcXEaM2aMkpOT1b59e23atEkTJ06scj8TJ07U2LFjNWnSJMXExGjw4MGOMUFWq1VfffWVbrnlFg0cOFAxMTEaPny4CgsLy40tqm7z5s3TmTNn1KFDBz3++OMaNWqUGjdu7NQmPT1da9askc1mU4cOHSrsZ9SoURo7dqzGjh2rO+64QytXrtSKFSsUFRVVLXU/9dRTGjhwoAYPHqy77rpLp06d0siRI53aDB06VJmZmXrnnXfUpk0bPfjgg05XAyvar4cfflgzZszQ9OnT1aZNG7377ruaP3++evbsWS37AXiKxTB+9d1ZAACAGxxXfgAAgKkQfgAAgKkQfgAAgKkQfgAAgKkQfgAAgKkQfgAAgKkQfgAAgKkQfgAAgKkQfgAAgKkQfgAAgKkQfgAAgKn8P52XM+EISMQGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_vif(df_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulation-based power analysis\n",
    "We are now fast rewinding and putting ourselves in the shoes of the experimenter *before* she actually starts collecting any data (well maybe these are actually your shoes).\n",
    "How can I know **how many subjects to include in my sample to detect the effect of interest (if it is indeed present)**. This is the goal of *power analyses*. Statistical power refers to the probability that the effect, if present, is detected. The larger the better. The larger the sample size the larger the power. But we don't want to collect data from an infinite cohort of subjects either...\n",
    "When your test of interest is a simple t-test or anova, then you can use a software ([G\\*Power](https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower)) or online tools to run your power analyses. When you want to use the result of a more complex statistical analysis (as here), you can used simulation-based power analysis, which is presented in the panel B of the figure below (from [this](https://link.springer.com/article/10.3758/s13428-022-01793-9) little guide to behavioral experiments in humans we wrote)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Task](https://media.springernature.com/full/springer-static/image/art%3A10.3758%2Fs13428-022-01793-9/MediaObjects/13428_2022_1793_Fig2_HTML.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here **we will calculate the statistical power for detecting the confirmation bias in the dataset**. We will switch to the numerosity judgment dataset (of the same paper), the one we used in Assignment 2 to test for the confirmation bias.\n",
    "\n",
    "The code below loads the data, changes the reference of the variable to 50, splits the second stimulus into two regressors `x2con` and `x2inc` depending on whether they are consistent with the intermediate binary classification performed by the subject. Just as we did in Assignment 2 (note that you don't really need to delve back into the intricacies of this regression model to follow the logic of the power analysis. It's just a linear regression model, that's about all we need to know.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>binchoice</th>\n",
       "      <th>x2</th>\n",
       "      <th>xavg</th>\n",
       "      <th>estim</th>\n",
       "      <th>subj</th>\n",
       "      <th>consistent</th>\n",
       "      <th>x2con</th>\n",
       "      <th>x2inc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.625</td>\n",
       "      <td>-1</td>\n",
       "      <td>-8.375</td>\n",
       "      <td>-6.5000</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>-8.375</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.500</td>\n",
       "      <td>1</td>\n",
       "      <td>-11.375</td>\n",
       "      <td>-6.9375</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-11.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.250</td>\n",
       "      <td>1</td>\n",
       "      <td>7.500</td>\n",
       "      <td>2.6250</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7.500</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.375</td>\n",
       "      <td>-1</td>\n",
       "      <td>-11.625</td>\n",
       "      <td>-7.5000</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>-11.625</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.750</td>\n",
       "      <td>-1</td>\n",
       "      <td>-8.625</td>\n",
       "      <td>-5.6875</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>-8.625</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x1  binchoice      x2    xavg  estim  subj  consistent   x2con   x2inc\n",
       "0 -4.625         -1  -8.375 -6.5000     44     1        True  -8.375  -0.000\n",
       "1 -2.500          1 -11.375 -6.9375     41     1       False  -0.000 -11.375\n",
       "2 -2.250          1   7.500  2.6250     54     1        True   7.500   0.000\n",
       "3 -3.375         -1 -11.625 -7.5000     40     1        True -11.625  -0.000\n",
       "4 -2.750         -1  -8.625 -5.6875     45     1        True  -8.625  -0.000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the data\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/wimmerlab/MBC-DataAnalysis/main/A2_LinearRegression/Task_Numerical.csv\",sep=',')\n",
    "\n",
    "# use 50 as reference point\n",
    "df.x1 = df.x1 - 50\n",
    "df.x2 = df.x2 - 50\n",
    "df.xavg = df.xavg - 50\n",
    "\n",
    "# sign of x2\n",
    "sgn = np.sign(df['x2'])\n",
    "\n",
    "# define boolean array for consistency\n",
    "consistent = sgn ==df['binchoice']\n",
    "\n",
    "# add to dataframe\n",
    "df['consistent'] = consistent\n",
    "\n",
    "# x2con: equal to x2 if consistent, 0 otherwise\n",
    "x2con = df['x2'] * consistent\n",
    "\n",
    "# x2inc: equal to x2 if inconsistent, 0 otherwise \n",
    "x2inc = df['x2'] * ~consistent # could use (1-consistent) instead of ~consistent\n",
    "\n",
    "# add as variables in dataframe\n",
    "df['x2con'] = x2con\n",
    "df['x2inc'] = x2inc\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to run a simulation-based power analysis we need to be able to simulate the model. Makes sense. Here I adapted the code we used to simulate the simple linear regression model to the case of multiple regression (so multiple regressors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import function\n",
    "from numpy.random import normal\n",
    "\n",
    "# define function that simulates the linear regression model\n",
    "def generate_multiple_linear_model(x, intercept, weights, sigma):\n",
    "    \"\"\"\n",
    "    Simulates a multiple linear model \n",
    "    Args:\n",
    "       X (array): value(s) of the regressor\n",
    "       intercept (float): intercept\n",
    "       weights (array): weights for all regressors\n",
    "       sigma (float): standard deviation of noise parameter\n",
    "    \"\"\"  \n",
    "    # use linear relationship (using matrix multiplication)\n",
    "    y = intercept + np.matmul(x,weights)\n",
    "    \n",
    "    # add gaussian noise with standard deviation=sigma\n",
    "    n = x.shape[0]\n",
    "    y = y + sigma*normal(size=n)\n",
    "    \n",
    "    # output the result\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us simulate the model**. For this, we need to specify model parameters. This is the weak part of power analysis: you need to know the true values of the model parameters to know how likely you are to detect an effect... sounds a bit circular. So, in general we use plausible values, either from the literature or from pilot participants. The most crucial choice is the strength of the effect (so here the difference between the weights of the second stimulus when it's consistent vs. inconsistent with the intermediate choice, `w_x2con` and `w_x2inc`). Needless to say, the larger this difference, the more probable it is to be detected statistically. Here we test with a difference of .1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_size = .1 # size of difference between weights of second stimulus depending on its consistency with intermediate choice\n",
    "w_x1 = .5 # weight for the first stimulus\n",
    "w_x2con = .5 # weight for the second stimulus, if consistent\n",
    "w_x2inc = .5 - effect_size # weight for the second stimulus, if inconsistent\n",
    "intercept = 50 # intercept (50 because this is the reference point, corresponds to an unbiased model)\n",
    "\n",
    "sigma = 5 # value of the gaussian noise\n",
    "\n",
    "weights = [w_x1,w_x2con,w_x2inc] # array of weights\n",
    "\n",
    "# define design matrix, i.e. put all regressors into a numpy array\n",
    "X = df[['x1','x2con','x2inc']]\n",
    "X = X.to_numpy()\n",
    "\n",
    "# simulate!\n",
    "y_simul = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation is the first part of the process. The second part is **fitting the regression model on the simulated data and applying the exact same statistical test that we want to apply on the true experimental data**. Here the test was a paired t-test between the set of consistent/inconsistent weights for the second stimulus, across subjects. I have copied the code from Assignment 2 into a function that takes an experimental dataframe (either from experimental or simulated data) as input and spits out the p-value for the corresponding test. Basically it tells us whether the effect is statistically significant in any dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing library \n",
    "import scipy.stats as stats\n",
    "\n",
    "# define function that provides p-value for effect of confirmation bias in given dataset\n",
    "def fit_confirmation_bias_model(df):\n",
    "    \"\"\"\n",
    "    Fits the regression model for confirmation bias \n",
    "    Args:\n",
    "       df (dataframe): experimental dataset\n",
    "    \"\"\"  \n",
    "\n",
    "    #Â list of subjects\n",
    "    subjects = np.unique(df.subj)\n",
    "\n",
    "    # create array for weights for each subject (how many weights per subject?)\n",
    "    pars = np.zeros((len(subjects),4))\n",
    "\n",
    "    # loop through all subjects\n",
    "    for i,s in enumerate(subjects):\n",
    "\n",
    "        # dataframe for this subject\n",
    "        df_subj = df[df.subj==s]\n",
    "\n",
    "        # define and fit regression model\n",
    "        mod = ols(formula = 'estim ~ x1 + x2con + x2inc', data=df_subj)\n",
    "        res = mod.fit()\n",
    "\n",
    "        # store values of the parameters\n",
    "        pars[i,:] = res.params\n",
    "\n",
    "    # compute mean across subjects\n",
    "    mean_weights = np.mean(pars,axis=0)\n",
    "\n",
    "    T = stats.ttest_rel(pars[:,2], pars[:,3]) \n",
    "\n",
    "    return  T.pvalue, pars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us plug the two processes: we fit the regression models on the synthetic dataset (from simulations) and compute the corresponding p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value 0.006542304873937733\n"
     ]
    }
   ],
   "source": [
    "# copy the experimental dataframe\n",
    "df_simul = df\n",
    "\n",
    "# copy the value of the simulated model as if it were the estimation from the subject\n",
    "df_simul['estim'] = ??\n",
    "\n",
    "# compute the pvalue for the confirmation bias on this dataset\n",
    "pval, est_weights = ???\n",
    "\n",
    "print(\"p-value\",??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We're almost there. You can simulate the model again by running the corresponding cell above and see that the p-value is different everytime. Sometimes it is significant, but not always. Let us do it in a consistent way: **build a function that repeatedly simulates a synthetic dataset and estimate the significance of the confirmation bias in this synthetic dataset, and outputs the collection of p-values for all simulated datasets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that simulates and fits model multiple time\n",
    "def simulate_and_fit_confirmation_bias_model(df, intercept, weights, sigma, nSimul=100):\n",
    "    \"\"\"\n",
    "    Simulates and test for the confirmation bias, for a given number of synthetic dataset\n",
    "    Args:\n",
    "       df (dataframe): experimental dataset\n",
    "       intercept (float): value of intercept\n",
    "       weights (array of 3): value of the weights\n",
    "       sigma (float): std of observation noise (gaussian)\n",
    "       nSimul (integer): number of datasets\n",
    "    \"\"\"  \n",
    "    # p-values for all datasets\n",
    "    all_pvalues = []\n",
    "    for s in range(??): # for all datasets\n",
    "        \n",
    "        # simulate model\n",
    "        y_simul = ??\n",
    "\n",
    "        # add simulated behavior as dependent variable in the dataframe\n",
    "        ??\n",
    "        \n",
    "        # test for confirmation bias in the dataset\n",
    "        pval, _ = ??\n",
    "\n",
    "        # add p-value\n",
    "        all_pvalues.append(??)\n",
    "        \n",
    "    all_pvalues = np.array(all_pvalues) # convert to numpy\n",
    "    return all_pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's run this (this should take around a minute or so)! **Plot the histogram of p-values and add a vertical line for the significance criterion (at 0.05)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# simulate all datasets and compute corresponding p-values\n",
    "all_p = ???\n",
    "\n",
    "# add histogram\n",
    "bins = .05*np.arange(21) # bins: 0, 0.05, .1, ...\n",
    "plt.hist(??, bins=bins);\n",
    "\n",
    "# add vertical line\n",
    "plt.vlines(.05,0, 70,color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, **compute the statistical power for this analysis**, i.e. the proportion of simulations that yield a significant effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistical power:  75.0 %\n"
     ]
    }
   ],
   "source": [
    "# define boolean array for significance\n",
    "signif = (??)\n",
    "\n",
    "# compute power (i.e average of significant datasets)\n",
    "power = ??\n",
    "\n",
    "print(\"statistical power: \", 100*power, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you interpret this result?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*According to our simulations, if the effect is present with the same size as given by the parameters, there is about 72% probability to detect it in a 21-subject cohort (same as true experimental dataset). In general, we aim at statistical powers about 80-90% to maximize our chance to not miss the effect we're after.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check that the statistical power decreases if you decrease the effect size, and increase if your increase it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Linear mixed models\n",
    "In *linear mixed models* (or any mixed model), we consider subjects as *random factors*, i.e. factors that are not of interest per se but that contribute to the variability. Instead of estimating one parameter per subject, these models directly estimate the (co)variance of the weights across the population (assuming, again, Gaussian statistics). In this way, one parameter of variance will take care of the variability of a given weight across participants. Linear models that include both fixed factors (the baseline effect in the whole population - what we are generally interested in) and random factors (a variance parameter accounting for a family of regressors) are called \"mixed models\". These are usually the way that we should model subjects in our designs. Mixed models in the case of linear regression are called linear mixed models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed models are not very well developed in Python. The R language is a lot better adapted for advanced statistics methods. You have two choices:\n",
    "1) Use the function *mixedlm* in the Python library **statsmodels.formula.api**. This should be very parallel to what you just did, but you need to choose the right options for *mixedlm*\n",
    "\n",
    "2) Use the Python library **pymer4.models**, which gives you access to R functions from within Python. You can then import the function *Lmer* and call it specifying the fixed and random factors in your design (check the documentation). This is a more powerful approach, because of the simplicity of the syntax and all the power of R that you can invoke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Let us **implement a linear mixed model where the random factor for the intercept of each participant**. This is very similar to a traditional linear regression model with subject-specific biases (i.e. adding `C(subj)` in the formula), except that now we directly estimate the distribution of biases across the population, rather than each subject bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:46:55.260351Z",
     "start_time": "2022-02-04T22:46:54.971773Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "OawmGQU2M0FF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td>Model:</td>       <td>MixedLM</td> <td>Dependent Variable:</td>    <td>estim</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>No. Observations:</td>  <td>1738</td>         <td>Method:</td>          <td>REML</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>No. Groups:</td>      <td>21</td>          <td>Scale:</td>          <td>24.5893</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Min. group size:</td>    <td>72</td>      <td>Log-Likelihood:</td>   <td>-5254.8194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Max. group size:</td>    <td>84</td>        <td>Converged:</td>          <td>Yes</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Mean group size:</td>   <td>82.8</td>            <td></td>                <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>       <th>Coef.</th> <th>Std.Err.</th>    <th>z</th>    <th>P>|z|</th> <th>[0.025</th> <th>0.975]</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>49.864</td>   <td>0.126</td>  <td>394.759</td> <td>0.000</td> <td>49.616</td> <td>50.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>         <td>0.482</td>   <td>0.034</td>  <td>14.273</td>  <td>0.000</td>  <td>0.416</td>  <td>0.549</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>         <td>0.455</td>   <td>0.017</td>  <td>26.801</td>  <td>0.000</td>  <td>0.421</td>  <td>0.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Group Var</th>  <td>0.017</td>   <td>0.020</td>     <td></td>       <td></td>       <td></td>       <td></td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "         Mixed Linear Model Regression Results\n",
       "========================================================\n",
       "Model:            MixedLM Dependent Variable: estim     \n",
       "No. Observations: 1738    Method:             REML      \n",
       "No. Groups:       21      Scale:              24.5893   \n",
       "Min. group size:  72      Log-Likelihood:     -5254.8194\n",
       "Max. group size:  84      Converged:          Yes       \n",
       "Mean group size:  82.8                                  \n",
       "--------------------------------------------------------\n",
       "             Coef.  Std.Err.    z    P>|z| [0.025 0.975]\n",
       "--------------------------------------------------------\n",
       "Intercept    49.864    0.126 394.759 0.000 49.616 50.111\n",
       "x1            0.482    0.034  14.273 0.000  0.416  0.549\n",
       "x2            0.455    0.017  26.801 0.000  0.421  0.488\n",
       "Group Var     0.017    0.020                            \n",
       "========================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import package\n",
    "from statsmodels.formula.api import mixedlm\n",
    "\n",
    "# define model: \n",
    "# - the fixed effects are given in the formula as for ols\n",
    "# - the random factor is defined by 'groups' (variable 'subj')\n",
    "# - the random effects (which weights should vary per subject) is defined by re_formula (here, just the intercept so \"~1\")\n",
    "mod_sub = mixedlm(formula='estim ~ x1 + x2', data=df, groups=df['subj'], re_formula=\"~1\")\n",
    "\n",
    "# fit the model\n",
    "res_sub = mod_sub.fit(maxiter=500,method=\"nm\", reml=\"false\")\n",
    "\n",
    "# show summary\n",
    "res_sub.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model gives us a similar result to what we got when we modeled subjects as fixed factors in Assignment 2. But actually subjects will differ not only because of their overall bias, but also because of a different weighting of stimulus information. So a better model will incorporate this variability, by adding the first and second stimuli as random factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:46:56.059655Z",
     "start_time": "2022-02-04T22:46:55.261765Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td>Model:</td>       <td>MixedLM</td> <td>Dependent Variable:</td>    <td>estim</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>No. Observations:</td>  <td>1738</td>         <td>Method:</td>          <td>REML</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>No. Groups:</td>      <td>21</td>          <td>Scale:</td>          <td>24.5622</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Min. group size:</td>    <td>72</td>      <td>Log-Likelihood:</td>   <td>-5254.8435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Max. group size:</td>    <td>84</td>        <td>Converged:</td>          <td>Yes</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Mean group size:</td>   <td>82.8</td>            <td></td>                <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>         <th>Coef.</th> <th>Std.Err.</th>    <th>z</th>    <th>P>|z|</th> <th>[0.025</th> <th>0.975]</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>      <td>49.864</td>   <td>0.128</td>  <td>389.007</td> <td>0.000</td> <td>49.613</td> <td>50.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>              <td>0.482</td>   <td>0.034</td>  <td>14.224</td>  <td>0.000</td>  <td>0.416</td>  <td>0.549</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>              <td>0.455</td>   <td>0.017</td>  <td>26.171</td>  <td>0.000</td>  <td>0.421</td>  <td>0.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Group Var</th>       <td>0.027</td>   <td>0.039</td>     <td></td>       <td></td>       <td></td>       <td></td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Group x x1 Cov</th> <td>-0.002</td>   <td>0.009</td>     <td></td>       <td></td>       <td></td>       <td></td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1 Var</th>          <td>0.000</td>   <td>0.003</td>     <td></td>       <td></td>       <td></td>       <td></td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Group x x2 Cov</th>  <td>0.001</td>   <td>0.004</td>     <td></td>       <td></td>       <td></td>       <td></td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1 x x2 Cov</th>    <td>-0.000</td>   <td>0.001</td>     <td></td>       <td></td>       <td></td>       <td></td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2 Var</th>          <td>0.000</td>   <td>0.001</td>     <td></td>       <td></td>       <td></td>       <td></td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "          Mixed Linear Model Regression Results\n",
       "==========================================================\n",
       "Model:              MixedLM Dependent Variable: estim     \n",
       "No. Observations:   1738    Method:             REML      \n",
       "No. Groups:         21      Scale:              24.5622   \n",
       "Min. group size:    72      Log-Likelihood:     -5254.8435\n",
       "Max. group size:    84      Converged:          Yes       \n",
       "Mean group size:    82.8                                  \n",
       "----------------------------------------------------------\n",
       "               Coef.  Std.Err.    z    P>|z| [0.025 0.975]\n",
       "----------------------------------------------------------\n",
       "Intercept      49.864    0.128 389.007 0.000 49.613 50.115\n",
       "x1              0.482    0.034  14.224 0.000  0.416  0.549\n",
       "x2              0.455    0.017  26.171 0.000  0.421  0.489\n",
       "Group Var       0.027    0.039                            \n",
       "Group x x1 Cov -0.002    0.009                            \n",
       "x1 Var          0.000    0.003                            \n",
       "Group x x2 Cov  0.001    0.004                            \n",
       "x1 x x2 Cov    -0.000    0.001                            \n",
       "x2 Var          0.000    0.001                            \n",
       "==========================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.formula.api import mixedlm\n",
    "\n",
    "# same as above, but now the random effects include x1 and x2\n",
    "mod_sub = mixedlm(formula='estim ~ x1 + x2', data=df, groups=df['subj'], re_formula=\"~x1+x2\")\n",
    "res_sub = mod_sub.fit(maxiter=500,method=\"nm\", reml=\"false\")\n",
    "\n",
    "res_sub.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) we can repeat this analysis using the other method, which calls R from Python. Convince yourself that the results are similar (Note: installing pymer 4 may not work on all distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T22:47:00.607034Z",
     "start_time": "2022-02-04T22:46:56.061464Z"
    }
   },
   "outputs": [],
   "source": [
    "from pymer4.models import Lmer\n",
    "\n",
    "model = Lmer('estim ~ x1 + x2 + (1 + x1 + x2|subj)', data=df)\n",
    "res = model.fit()\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "assignment_4-full.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
